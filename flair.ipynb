{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flair.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudolmat/SAKI-Exercise1/blob/master/flair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjL7I3UFDyb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37L1rY_YWRBF",
        "colab_type": "code",
        "outputId": "00077943-6c47-4d45-c20f-a34f77d38e7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHj_2UbdWi63",
        "colab_type": "code",
        "outputId": "70436932-4469-4832-ebd4-d8f3e587039b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "cd /content/gdrive/My Drive/SAKI/Exercise_2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/SAKI/Exercise_2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csQfFaw6nIYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/SAKI/Exercise_2\") "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrJ6fq1onfwL",
        "colab_type": "code",
        "outputId": "032f3f5e-6e12-4001-be54-fa8b43d21df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flair in /usr/local/lib/python3.6/dist-packages (0.4.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from flair) (2018.1.10)\n",
            "Requirement already satisfied: deprecated>=1.2.4 in /usr/local/lib/python3.6/dist-packages (from flair) (1.2.5)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair) (4.28.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair) (3.0.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from flair) (0.0)\n",
            "Requirement already satisfied: pytest>=3.6.4 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.4)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair) (3.6.0)\n",
            "Requirement already satisfied: mpld3==0.3 in /usr/local/lib/python3.6/dist-packages (from flair) (0.3)\n",
            "Requirement already satisfied: sqlitedict>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.6.0)\n",
            "Requirement already satisfied: pytorch-pretrained-bert>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.6.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from flair) (1.1.0)\n",
            "Requirement already satisfied: bpemb>=0.2.9 in /usr/local/lib/python3.6/dist-packages (from flair) (0.2.12)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from flair) (0.8.3)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.20 in /usr/local/lib/python3.6/dist-packages (from flair) (1.24.3)\n",
            "Requirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair) (0.1.2)\n",
            "Requirement already satisfied: segtok>=1.5.7 in /usr/local/lib/python3.6/dist-packages (from flair) (1.5.7)\n",
            "Requirement already satisfied: wrapt<2,>=1 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair) (1.10.11)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair) (1.16.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->flair) (0.21.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (41.0.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.8.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (0.7.1)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (19.1.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest>=3.6.4->flair) (7.0.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.8.4)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim>=3.4.0->flair) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (2.21.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.1->flair) (1.9.162)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from bpemb>=0.2.9->flair) (0.1.82)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (2.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair) (3.8.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn->flair) (0.13.2)\n",
            "Requirement already satisfied: boto>=2.32 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim>=3.4.0->flair) (2.49.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2019.3.9)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert>=0.6.1->flair) (2.8)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.162 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert>=0.6.1->flair) (1.12.162)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.0)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.162->boto3->pytorch-pretrained-bert>=0.6.1->flair) (0.14)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ob5ShAjnqya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# imports \n",
        "from flair.data import Corpus\n",
        "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
        "from typing import List\n",
        "\n",
        "# columns of \"gold standard\" ner annotations and text\n",
        "columns = {3: 'text', 1: 'ner'}\n",
        "\n",
        "# folder where training and test data are\n",
        "data_folder = '/content/gdrive/My Drive/SAKI/Exercise_2'\n",
        "\n",
        "# 2. what tag do we want to predict?\n",
        "tag_type = 'ner'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3EW0796n05e",
        "colab_type": "code",
        "outputId": "72564741-6e95-42bf-f4ad-245325a13586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        }
      },
      "source": [
        "downsample = 1.0 # 1.0 is full data, try a much smaller number like 0.01 to test run the code\n",
        "# 1. get the corpus\n",
        "corpus: Corpus = NLPTaskDataFetcher.load_column_corpus(data_folder, columns,\n",
        "                                                              train_file='train_res_bilou.txt',\n",
        "                                                              test_file='test_res_bilou.txt',\n",
        "                                                              dev_file=None).downsample(downsample)\n",
        "\n",
        "print(corpus)\n",
        "\n",
        "# 3. make the tag dictionary from the corpus\n",
        "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
        "print(tag_dictionary.idx2item)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-08 09:50:42,402 Reading data from /content/gdrive/My Drive/SAKI/Exercise_2\n",
            "2019-06-08 09:50:42,405 Train: /content/gdrive/My Drive/SAKI/Exercise_2/train_res_bilou.txt\n",
            "2019-06-08 09:50:42,407 Dev: None\n",
            "2019-06-08 09:50:42,409 Test: /content/gdrive/My Drive/SAKI/Exercise_2/test_res_bilou.txt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:312: DeprecationWarning: Call to deprecated function (or staticmethod) read_column_data. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  train_file, column_format\n",
            "/usr/local/lib/python3.6/dist-packages/flair/data_fetcher.py:318: DeprecationWarning: Call to deprecated function (or staticmethod) read_column_data. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n",
            "  test_file, column_format\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Corpus: 28487 train + 3165 dev + 13050 test sentences\n",
            "[b'<unk>', b'O', b'B-Degree', b'I-Degree', b'B-Designation', b'I-Designation', b'\"B-Companies', b'\"I-Companies', b'<START>', b'<STOP>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxdWGKaaZaIG",
        "colab_type": "code",
        "outputId": "3f71f8fb-5b58-43a9-a020-f04679757987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "# 4. initialize embeddings. Experiment with different embedding types to see what gets the best results\n",
        "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings,FlairEmbeddings\n",
        "\n",
        "embedding_types: List[TokenEmbeddings] = [\n",
        "    WordEmbeddings('glove'),\n",
        "    # comment in this line to use character embeddings\n",
        "    # CharacterEmbeddings(),\n",
        "\n",
        "    # comment in these lines to use flair embeddings (needs a LONG time to train :-)\n",
        "    FlairEmbeddings('news-forward'),\n",
        "    FlairEmbeddings('news-backward'),\n",
        "]\n",
        "\n",
        "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
        "\n",
        "# 5. initialize sequence tagger\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
        "                                        embeddings=embeddings,\n",
        "                                        tag_dictionary=tag_dictionary,\n",
        "                                        tag_type=tag_type,\n",
        "                                        use_crf=True)\n",
        "\n",
        "\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOWqjoIaZyVg",
        "colab_type": "code",
        "outputId": "263a5382-2bee-4ccf-ff79-cda166a8ea43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 26187
        }
      },
      "source": [
        "# 6. initialize trainer\n",
        "from flair.trainers import ModelTrainer\n",
        "\n",
        "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
        "\n",
        "# 7. start training\n",
        "trainer.train('resources/taggers/resume-ner',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              max_epochs=150)\n",
        "\n",
        "# 8. plot training curves (optional)\n",
        "#from flair.visual.training_curves import Plotter\n",
        "#plotter = Plotter()\n",
        "#plotter.plot_training_curves('resources/taggers/example-ner/loss.tsv')\n",
        "#plotter.plot_weights('resources/taggers/example-ner/weights.txt')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-08 09:51:27,860 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 09:51:27,862 Evaluation method: MICRO_F1_SCORE\n",
            "2019-06-08 09:51:28,429 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 09:51:29,587 epoch 1 - iter 0/891 - loss 18.26095390\n",
            "2019-06-08 09:51:49,990 epoch 1 - iter 89/891 - loss 1.38068662\n",
            "2019-06-08 09:52:10,779 epoch 1 - iter 178/891 - loss 0.94589162\n",
            "2019-06-08 09:52:34,235 epoch 1 - iter 267/891 - loss 0.81468068\n",
            "2019-06-08 09:52:53,892 epoch 1 - iter 356/891 - loss 0.70975846\n",
            "2019-06-08 09:53:13,557 epoch 1 - iter 445/891 - loss 0.64924223\n",
            "2019-06-08 09:53:35,489 epoch 1 - iter 534/891 - loss 0.61052196\n",
            "2019-06-08 09:53:54,956 epoch 1 - iter 623/891 - loss 0.57770874\n",
            "2019-06-08 09:54:14,353 epoch 1 - iter 712/891 - loss 0.54459399\n",
            "2019-06-08 09:54:33,919 epoch 1 - iter 801/891 - loss 0.52175227\n",
            "2019-06-08 09:54:55,558 epoch 1 - iter 890/891 - loss 0.49316573\n",
            "2019-06-08 09:54:56,511 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 09:54:56,515 EPOCH 1 done: loss 0.4932 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 09:55:19,117 DEV : loss 0.27771225571632385 - score 0.8439\n",
            "2019-06-08 09:56:51,781 TEST : loss 0.2316266894340515 - score 0.8555\n",
            "2019-06-08 09:56:57,382 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 09:56:58,540 epoch 2 - iter 0/891 - loss 0.13595337\n",
            "2019-06-08 09:57:21,594 epoch 2 - iter 89/891 - loss 0.27284366\n",
            "2019-06-08 09:57:41,488 epoch 2 - iter 178/891 - loss 0.25933351\n",
            "2019-06-08 09:58:01,294 epoch 2 - iter 267/891 - loss 0.25940493\n",
            "2019-06-08 09:58:20,827 epoch 2 - iter 356/891 - loss 0.27007509\n",
            "2019-06-08 09:58:42,515 epoch 2 - iter 445/891 - loss 0.26310228\n",
            "2019-06-08 09:59:02,195 epoch 2 - iter 534/891 - loss 0.25951656\n",
            "2019-06-08 09:59:21,816 epoch 2 - iter 623/891 - loss 0.25236359\n",
            "2019-06-08 09:59:43,093 epoch 2 - iter 712/891 - loss 0.24651649\n",
            "2019-06-08 10:00:02,534 epoch 2 - iter 801/891 - loss 0.24679734\n",
            "2019-06-08 10:00:21,632 epoch 2 - iter 890/891 - loss 0.24691934\n",
            "2019-06-08 10:00:22,625 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:00:22,626 EPOCH 2 done: loss 0.2469 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 10:00:48,290 DEV : loss 0.20235037803649902 - score 0.8882\n",
            "2019-06-08 10:02:22,195 TEST : loss 0.16240936517715454 - score 0.8981\n",
            "2019-06-08 10:02:27,855 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:02:29,031 epoch 3 - iter 0/891 - loss 0.74398470\n",
            "2019-06-08 10:02:49,586 epoch 3 - iter 89/891 - loss 0.25176976\n",
            "2019-06-08 10:03:09,949 epoch 3 - iter 178/891 - loss 0.23175397\n",
            "2019-06-08 10:03:32,073 epoch 3 - iter 267/891 - loss 0.23726859\n",
            "2019-06-08 10:03:51,650 epoch 3 - iter 356/891 - loss 0.22692248\n",
            "2019-06-08 10:04:11,382 epoch 3 - iter 445/891 - loss 0.22639663\n",
            "2019-06-08 10:04:32,933 epoch 3 - iter 534/891 - loss 0.22096140\n",
            "2019-06-08 10:04:52,145 epoch 3 - iter 623/891 - loss 0.22277867\n",
            "2019-06-08 10:05:11,322 epoch 3 - iter 712/891 - loss 0.21418334\n",
            "2019-06-08 10:05:30,520 epoch 3 - iter 801/891 - loss 0.20964347\n",
            "2019-06-08 10:05:51,518 epoch 3 - iter 890/891 - loss 0.20867130\n",
            "2019-06-08 10:05:52,498 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:05:52,500 EPOCH 3 done: loss 0.2087 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 10:06:14,844 DEV : loss 0.1776428371667862 - score 0.8849\n",
            "2019-06-08 10:07:46,030 TEST : loss 0.14932182431221008 - score 0.8987\n",
            "2019-06-08 10:07:46,036 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:07:47,010 epoch 4 - iter 0/891 - loss 0.02489568\n",
            "2019-06-08 10:08:09,415 epoch 4 - iter 89/891 - loss 0.17295960\n",
            "2019-06-08 10:08:28,851 epoch 4 - iter 178/891 - loss 0.17919299\n",
            "2019-06-08 10:08:48,243 epoch 4 - iter 267/891 - loss 0.18153634\n",
            "2019-06-08 10:09:09,480 epoch 4 - iter 356/891 - loss 0.18681623\n",
            "2019-06-08 10:09:28,556 epoch 4 - iter 445/891 - loss 0.18829443\n",
            "2019-06-08 10:09:47,808 epoch 4 - iter 534/891 - loss 0.19173723\n",
            "2019-06-08 10:10:08,845 epoch 4 - iter 623/891 - loss 0.19125661\n",
            "2019-06-08 10:10:27,815 epoch 4 - iter 712/891 - loss 0.19208810\n",
            "2019-06-08 10:10:46,731 epoch 4 - iter 801/891 - loss 0.19136437\n",
            "2019-06-08 10:11:05,863 epoch 4 - iter 890/891 - loss 0.18775614\n",
            "2019-06-08 10:11:06,839 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:11:06,843 EPOCH 4 done: loss 0.1878 - lr 0.1000 - bad epochs 1\n",
            "2019-06-08 10:11:31,725 DEV : loss 0.18256345391273499 - score 0.8776\n",
            "2019-06-08 10:13:02,617 TEST : loss 0.14331766963005066 - score 0.8972\n",
            "2019-06-08 10:13:02,624 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:13:03,643 epoch 5 - iter 0/891 - loss 0.09872843\n",
            "2019-06-08 10:13:23,187 epoch 5 - iter 89/891 - loss 0.17480109\n",
            "2019-06-08 10:13:45,287 epoch 5 - iter 178/891 - loss 0.16260302\n",
            "2019-06-08 10:14:04,536 epoch 5 - iter 267/891 - loss 0.17166627\n",
            "2019-06-08 10:14:23,458 epoch 5 - iter 356/891 - loss 0.16641697\n",
            "2019-06-08 10:14:44,594 epoch 5 - iter 445/891 - loss 0.16535321\n",
            "2019-06-08 10:15:03,661 epoch 5 - iter 534/891 - loss 0.17106757\n",
            "2019-06-08 10:15:22,879 epoch 5 - iter 623/891 - loss 0.16968488\n",
            "2019-06-08 10:15:42,062 epoch 5 - iter 712/891 - loss 0.17048951\n",
            "2019-06-08 10:16:03,103 epoch 5 - iter 801/891 - loss 0.17350387\n",
            "2019-06-08 10:16:22,055 epoch 5 - iter 890/891 - loss 0.17460592\n",
            "2019-06-08 10:16:23,019 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:16:23,025 EPOCH 5 done: loss 0.1746 - lr 0.1000 - bad epochs 2\n",
            "2019-06-08 10:16:44,999 DEV : loss 0.1698070615530014 - score 0.8935\n",
            "2019-06-08 10:18:18,056 TEST : loss 0.12877093255519867 - score 0.907\n",
            "2019-06-08 10:18:23,518 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:18:24,679 epoch 6 - iter 0/891 - loss 0.07725935\n",
            "2019-06-08 10:18:44,832 epoch 6 - iter 89/891 - loss 0.17115595\n",
            "2019-06-08 10:19:04,595 epoch 6 - iter 178/891 - loss 0.17314510\n",
            "2019-06-08 10:19:24,126 epoch 6 - iter 267/891 - loss 0.17151362\n",
            "2019-06-08 10:19:45,566 epoch 6 - iter 356/891 - loss 0.16295417\n",
            "2019-06-08 10:20:04,569 epoch 6 - iter 445/891 - loss 0.16107575\n",
            "2019-06-08 10:20:23,809 epoch 6 - iter 534/891 - loss 0.15900735\n",
            "2019-06-08 10:20:44,678 epoch 6 - iter 623/891 - loss 0.16087550\n",
            "2019-06-08 10:21:03,616 epoch 6 - iter 712/891 - loss 0.16378159\n",
            "2019-06-08 10:21:22,661 epoch 6 - iter 801/891 - loss 0.16303952\n",
            "2019-06-08 10:21:43,973 epoch 6 - iter 890/891 - loss 0.16244183\n",
            "2019-06-08 10:21:44,957 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:21:44,964 EPOCH 6 done: loss 0.1624 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 10:22:07,312 DEV : loss 0.15927168726921082 - score 0.8854\n",
            "2019-06-08 10:23:39,123 TEST : loss 0.13047824800014496 - score 0.9049\n",
            "2019-06-08 10:23:39,133 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:23:40,120 epoch 7 - iter 0/891 - loss 0.03677915\n",
            "2019-06-08 10:23:59,727 epoch 7 - iter 89/891 - loss 0.15373812\n",
            "2019-06-08 10:24:21,896 epoch 7 - iter 178/891 - loss 0.15715431\n",
            "2019-06-08 10:24:41,283 epoch 7 - iter 267/891 - loss 0.15665019\n",
            "2019-06-08 10:25:00,483 epoch 7 - iter 356/891 - loss 0.15750453\n",
            "2019-06-08 10:25:21,783 epoch 7 - iter 445/891 - loss 0.15481324\n",
            "2019-06-08 10:25:40,979 epoch 7 - iter 534/891 - loss 0.15537753\n",
            "2019-06-08 10:26:00,145 epoch 7 - iter 623/891 - loss 0.15547759\n",
            "2019-06-08 10:26:21,417 epoch 7 - iter 712/891 - loss 0.15178469\n",
            "2019-06-08 10:26:41,008 epoch 7 - iter 801/891 - loss 0.15692339\n",
            "2019-06-08 10:26:59,849 epoch 7 - iter 890/891 - loss 0.15782577\n",
            "2019-06-08 10:27:00,801 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:27:00,808 EPOCH 7 done: loss 0.1578 - lr 0.1000 - bad epochs 1\n",
            "2019-06-08 10:27:23,026 DEV : loss 0.15292195975780487 - score 0.8883\n",
            "2019-06-08 10:28:58,657 TEST : loss 0.12687093019485474 - score 0.904\n",
            "2019-06-08 10:28:58,666 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:28:59,689 epoch 8 - iter 0/891 - loss 0.28391492\n",
            "2019-06-08 10:29:19,708 epoch 8 - iter 89/891 - loss 0.12358508\n",
            "2019-06-08 10:29:39,718 epoch 8 - iter 178/891 - loss 0.14100870\n",
            "2019-06-08 10:30:02,083 epoch 8 - iter 267/891 - loss 0.15721890\n",
            "2019-06-08 10:30:21,587 epoch 8 - iter 356/891 - loss 0.15723321\n",
            "2019-06-08 10:30:41,012 epoch 8 - iter 445/891 - loss 0.15432760\n",
            "2019-06-08 10:31:00,463 epoch 8 - iter 534/891 - loss 0.15274883\n",
            "2019-06-08 10:31:22,029 epoch 8 - iter 623/891 - loss 0.15113015\n",
            "2019-06-08 10:31:41,429 epoch 8 - iter 712/891 - loss 0.15072912\n",
            "2019-06-08 10:32:01,016 epoch 8 - iter 801/891 - loss 0.15081862\n",
            "2019-06-08 10:32:22,215 epoch 8 - iter 890/891 - loss 0.14931071\n",
            "2019-06-08 10:32:23,233 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:32:23,235 EPOCH 8 done: loss 0.1493 - lr 0.1000 - bad epochs 2\n",
            "2019-06-08 10:32:45,931 DEV : loss 0.1406189650297165 - score 0.8968\n",
            "2019-06-08 10:34:19,166 TEST : loss 0.12085738033056259 - score 0.9046\n",
            "2019-06-08 10:34:24,673 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:34:25,850 epoch 9 - iter 0/891 - loss 0.05584100\n",
            "2019-06-08 10:34:48,877 epoch 9 - iter 89/891 - loss 0.15281427\n",
            "2019-06-08 10:35:08,641 epoch 9 - iter 178/891 - loss 0.15135349\n",
            "2019-06-08 10:35:28,169 epoch 9 - iter 267/891 - loss 0.14623124\n",
            "2019-06-08 10:35:47,464 epoch 9 - iter 356/891 - loss 0.14479958\n",
            "2019-06-08 10:36:08,795 epoch 9 - iter 445/891 - loss 0.14552934\n",
            "2019-06-08 10:36:28,071 epoch 9 - iter 534/891 - loss 0.14147613\n",
            "2019-06-08 10:36:47,301 epoch 9 - iter 623/891 - loss 0.14051529\n",
            "2019-06-08 10:37:08,592 epoch 9 - iter 712/891 - loss 0.14164859\n",
            "2019-06-08 10:37:27,855 epoch 9 - iter 801/891 - loss 0.14021023\n",
            "2019-06-08 10:37:46,928 epoch 9 - iter 890/891 - loss 0.14289970\n",
            "2019-06-08 10:37:47,874 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:37:47,875 EPOCH 9 done: loss 0.1429 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 10:38:10,155 DEV : loss 0.13925564289093018 - score 0.9018\n",
            "2019-06-08 10:39:44,874 TEST : loss 0.11700722575187683 - score 0.9094\n",
            "2019-06-08 10:39:50,238 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:39:51,445 epoch 10 - iter 0/891 - loss 0.21525173\n",
            "2019-06-08 10:40:11,532 epoch 10 - iter 89/891 - loss 0.13009994\n",
            "2019-06-08 10:40:31,535 epoch 10 - iter 178/891 - loss 0.13008333\n",
            "2019-06-08 10:40:53,657 epoch 10 - iter 267/891 - loss 0.13470246\n",
            "2019-06-08 10:41:12,841 epoch 10 - iter 356/891 - loss 0.13619438\n",
            "2019-06-08 10:41:31,946 epoch 10 - iter 445/891 - loss 0.13335835\n",
            "2019-06-08 10:41:51,235 epoch 10 - iter 534/891 - loss 0.13725387\n",
            "2019-06-08 10:42:12,585 epoch 10 - iter 623/891 - loss 0.14018030\n",
            "2019-06-08 10:42:31,957 epoch 10 - iter 712/891 - loss 0.14353640\n",
            "2019-06-08 10:42:51,281 epoch 10 - iter 801/891 - loss 0.14284054\n",
            "2019-06-08 10:43:12,313 epoch 10 - iter 890/891 - loss 0.14325131\n",
            "2019-06-08 10:43:13,310 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:43:13,316 EPOCH 10 done: loss 0.1433 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 10:43:35,952 DEV : loss 0.13820068538188934 - score 0.9068\n",
            "2019-06-08 10:45:09,061 TEST : loss 0.11430846154689789 - score 0.9115\n",
            "2019-06-08 10:45:14,434 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:45:15,629 epoch 11 - iter 0/891 - loss 0.08907122\n",
            "2019-06-08 10:45:35,803 epoch 11 - iter 89/891 - loss 0.14057423\n",
            "2019-06-08 10:45:58,236 epoch 11 - iter 178/891 - loss 0.14017665\n",
            "2019-06-08 10:46:17,687 epoch 11 - iter 267/891 - loss 0.14472602\n",
            "2019-06-08 10:46:37,039 epoch 11 - iter 356/891 - loss 0.13784362\n",
            "2019-06-08 10:46:58,783 epoch 11 - iter 445/891 - loss 0.13912613\n",
            "2019-06-08 10:47:18,101 epoch 11 - iter 534/891 - loss 0.14646024\n",
            "2019-06-08 10:47:37,236 epoch 11 - iter 623/891 - loss 0.14597617\n",
            "2019-06-08 10:47:58,540 epoch 11 - iter 712/891 - loss 0.14328795\n",
            "2019-06-08 10:48:17,861 epoch 11 - iter 801/891 - loss 0.14014857\n",
            "2019-06-08 10:48:37,061 epoch 11 - iter 890/891 - loss 0.13793718\n",
            "2019-06-08 10:48:38,046 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:48:38,047 EPOCH 11 done: loss 0.1379 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 10:49:00,615 DEV : loss 0.13350018858909607 - score 0.9042\n",
            "2019-06-08 10:50:35,944 TEST : loss 0.11506864428520203 - score 0.908\n",
            "2019-06-08 10:50:35,953 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:50:36,993 epoch 12 - iter 0/891 - loss 0.01679435\n",
            "2019-06-08 10:50:56,714 epoch 12 - iter 89/891 - loss 0.13246895\n",
            "2019-06-08 10:51:16,664 epoch 12 - iter 178/891 - loss 0.13247740\n",
            "2019-06-08 10:51:38,962 epoch 12 - iter 267/891 - loss 0.12583281\n",
            "2019-06-08 10:51:58,402 epoch 12 - iter 356/891 - loss 0.12822235\n",
            "2019-06-08 10:52:17,915 epoch 12 - iter 445/891 - loss 0.13070864\n",
            "2019-06-08 10:52:37,490 epoch 12 - iter 534/891 - loss 0.13139278\n",
            "2019-06-08 10:52:58,826 epoch 12 - iter 623/891 - loss 0.12832844\n",
            "2019-06-08 10:53:17,920 epoch 12 - iter 712/891 - loss 0.13149519\n",
            "2019-06-08 10:53:37,320 epoch 12 - iter 801/891 - loss 0.13198980\n",
            "2019-06-08 10:53:58,487 epoch 12 - iter 890/891 - loss 0.13201363\n",
            "2019-06-08 10:53:59,482 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:53:59,484 EPOCH 12 done: loss 0.1320 - lr 0.1000 - bad epochs 1\n",
            "2019-06-08 10:54:22,073 DEV : loss 0.12754428386688232 - score 0.9117\n",
            "2019-06-08 10:55:55,481 TEST : loss 0.1101468876004219 - score 0.9149\n",
            "2019-06-08 10:56:00,942 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:56:02,151 epoch 13 - iter 0/891 - loss 0.11242829\n",
            "2019-06-08 10:56:25,258 epoch 13 - iter 89/891 - loss 0.11808704\n",
            "2019-06-08 10:56:45,015 epoch 13 - iter 178/891 - loss 0.11931671\n",
            "2019-06-08 10:57:04,475 epoch 13 - iter 267/891 - loss 0.12493881\n",
            "2019-06-08 10:57:23,973 epoch 13 - iter 356/891 - loss 0.12935125\n",
            "2019-06-08 10:57:45,499 epoch 13 - iter 445/891 - loss 0.13040661\n",
            "2019-06-08 10:58:04,743 epoch 13 - iter 534/891 - loss 0.13205709\n",
            "2019-06-08 10:58:23,777 epoch 13 - iter 623/891 - loss 0.12919042\n",
            "2019-06-08 10:58:45,156 epoch 13 - iter 712/891 - loss 0.12953923\n",
            "2019-06-08 10:59:04,532 epoch 13 - iter 801/891 - loss 0.12799745\n",
            "2019-06-08 10:59:23,676 epoch 13 - iter 890/891 - loss 0.12678090\n",
            "2019-06-08 10:59:24,667 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 10:59:24,669 EPOCH 13 done: loss 0.1268 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 10:59:47,233 DEV : loss 0.12685957551002502 - score 0.9112\n",
            "2019-06-08 11:01:21,663 TEST : loss 0.10936331748962402 - score 0.9166\n",
            "2019-06-08 11:01:21,672 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:01:22,653 epoch 14 - iter 0/891 - loss 0.16972791\n",
            "2019-06-08 11:01:42,571 epoch 14 - iter 89/891 - loss 0.12895869\n",
            "2019-06-08 11:02:02,628 epoch 14 - iter 178/891 - loss 0.13462338\n",
            "2019-06-08 11:02:24,754 epoch 14 - iter 267/891 - loss 0.13195736\n",
            "2019-06-08 11:02:44,224 epoch 14 - iter 356/891 - loss 0.12347916\n",
            "2019-06-08 11:03:03,308 epoch 14 - iter 445/891 - loss 0.12534377\n",
            "2019-06-08 11:03:24,622 epoch 14 - iter 534/891 - loss 0.12659684\n",
            "2019-06-08 11:03:43,870 epoch 14 - iter 623/891 - loss 0.12847329\n",
            "2019-06-08 11:04:02,889 epoch 14 - iter 712/891 - loss 0.12823773\n",
            "2019-06-08 11:04:24,136 epoch 14 - iter 801/891 - loss 0.12808447\n",
            "2019-06-08 11:04:43,307 epoch 14 - iter 890/891 - loss 0.12576266\n",
            "2019-06-08 11:04:44,276 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:04:44,277 EPOCH 14 done: loss 0.1258 - lr 0.1000 - bad epochs 1\n",
            "2019-06-08 11:05:06,837 DEV : loss 0.13488563895225525 - score 0.9126\n",
            "2019-06-08 11:06:39,249 TEST : loss 0.11100342869758606 - score 0.9182\n",
            "2019-06-08 11:06:44,715 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:06:45,885 epoch 15 - iter 0/891 - loss 0.05086231\n",
            "2019-06-08 11:07:08,810 epoch 15 - iter 89/891 - loss 0.11836567\n",
            "2019-06-08 11:07:28,463 epoch 15 - iter 178/891 - loss 0.12208619\n",
            "2019-06-08 11:07:47,970 epoch 15 - iter 267/891 - loss 0.12249394\n",
            "2019-06-08 11:08:09,286 epoch 15 - iter 356/891 - loss 0.11943820\n",
            "2019-06-08 11:08:28,377 epoch 15 - iter 445/891 - loss 0.11780892\n",
            "2019-06-08 11:08:47,708 epoch 15 - iter 534/891 - loss 0.11923272\n",
            "2019-06-08 11:09:06,877 epoch 15 - iter 623/891 - loss 0.12024342\n",
            "2019-06-08 11:09:27,963 epoch 15 - iter 712/891 - loss 0.11986640\n",
            "2019-06-08 11:09:47,245 epoch 15 - iter 801/891 - loss 0.11704281\n",
            "2019-06-08 11:10:06,390 epoch 15 - iter 890/891 - loss 0.11936879\n",
            "2019-06-08 11:10:07,341 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:10:07,349 EPOCH 15 done: loss 0.1194 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 11:10:32,619 DEV : loss 0.12337379902601242 - score 0.9114\n",
            "2019-06-08 11:12:04,926 TEST : loss 0.10828009247779846 - score 0.9184\n",
            "2019-06-08 11:12:04,934 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:12:05,975 epoch 16 - iter 0/891 - loss 0.07923108\n",
            "2019-06-08 11:12:26,137 epoch 16 - iter 89/891 - loss 0.12171615\n",
            "2019-06-08 11:12:48,541 epoch 16 - iter 178/891 - loss 0.13183326\n",
            "2019-06-08 11:13:07,970 epoch 16 - iter 267/891 - loss 0.12841199\n",
            "2019-06-08 11:13:27,250 epoch 16 - iter 356/891 - loss 0.12984781\n",
            "2019-06-08 11:13:46,471 epoch 16 - iter 445/891 - loss 0.12676824\n",
            "2019-06-08 11:14:07,809 epoch 16 - iter 534/891 - loss 0.12391366\n",
            "2019-06-08 11:14:27,075 epoch 16 - iter 623/891 - loss 0.12146648\n",
            "2019-06-08 11:14:46,305 epoch 16 - iter 712/891 - loss 0.12220331\n",
            "2019-06-08 11:15:07,665 epoch 16 - iter 801/891 - loss 0.12328923\n",
            "2019-06-08 11:15:26,823 epoch 16 - iter 890/891 - loss 0.12243655\n",
            "2019-06-08 11:15:27,781 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:15:27,785 EPOCH 16 done: loss 0.1224 - lr 0.1000 - bad epochs 1\n",
            "2019-06-08 11:15:50,192 DEV : loss 0.12191153317689896 - score 0.9117\n",
            "2019-06-08 11:17:24,743 TEST : loss 0.11049649119377136 - score 0.9175\n",
            "2019-06-08 11:17:24,752 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:17:25,773 epoch 17 - iter 0/891 - loss 0.05826679\n",
            "2019-06-08 11:17:45,684 epoch 17 - iter 89/891 - loss 0.10632329\n",
            "2019-06-08 11:18:05,700 epoch 17 - iter 178/891 - loss 0.10963043\n",
            "2019-06-08 11:18:25,324 epoch 17 - iter 267/891 - loss 0.11326724\n",
            "2019-06-08 11:18:47,085 epoch 17 - iter 356/891 - loss 0.11394690\n",
            "2019-06-08 11:19:06,216 epoch 17 - iter 445/891 - loss 0.11593713\n",
            "2019-06-08 11:19:25,400 epoch 17 - iter 534/891 - loss 0.11743422\n",
            "2019-06-08 11:19:46,878 epoch 17 - iter 623/891 - loss 0.11894527\n",
            "2019-06-08 11:20:06,015 epoch 17 - iter 712/891 - loss 0.12166459\n",
            "2019-06-08 11:20:25,200 epoch 17 - iter 801/891 - loss 0.12355725\n",
            "2019-06-08 11:20:44,206 epoch 17 - iter 890/891 - loss 0.12346700\n",
            "2019-06-08 11:20:45,162 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:20:45,164 EPOCH 17 done: loss 0.1235 - lr 0.1000 - bad epochs 2\n",
            "2019-06-08 11:21:10,449 DEV : loss 0.1312246471643448 - score 0.9064\n",
            "2019-06-08 11:22:42,816 TEST : loss 0.10939300805330276 - score 0.9148\n",
            "2019-06-08 11:22:42,826 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:22:43,858 epoch 18 - iter 0/891 - loss 0.04596940\n",
            "2019-06-08 11:23:03,665 epoch 18 - iter 89/891 - loss 0.13371518\n",
            "2019-06-08 11:23:26,079 epoch 18 - iter 178/891 - loss 0.12570228\n",
            "2019-06-08 11:23:45,637 epoch 18 - iter 267/891 - loss 0.11809493\n",
            "2019-06-08 11:24:04,696 epoch 18 - iter 356/891 - loss 0.12306871\n",
            "2019-06-08 11:24:26,080 epoch 18 - iter 445/891 - loss 0.12149293\n",
            "2019-06-08 11:24:45,329 epoch 18 - iter 534/891 - loss 0.12043266\n",
            "2019-06-08 11:25:04,392 epoch 18 - iter 623/891 - loss 0.11812435\n",
            "2019-06-08 11:25:23,498 epoch 18 - iter 712/891 - loss 0.11399706\n",
            "2019-06-08 11:25:44,806 epoch 18 - iter 801/891 - loss 0.11597603\n",
            "2019-06-08 11:26:04,043 epoch 18 - iter 890/891 - loss 0.11677689\n",
            "2019-06-08 11:26:05,062 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:26:05,066 EPOCH 18 done: loss 0.1168 - lr 0.1000 - bad epochs 3\n",
            "2019-06-08 11:26:27,451 DEV : loss 0.12728337943553925 - score 0.9138\n",
            "2019-06-08 11:28:01,358 TEST : loss 0.11941422522068024 - score 0.9026\n",
            "2019-06-08 11:28:06,794 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:28:08,010 epoch 19 - iter 0/891 - loss 0.03982925\n",
            "2019-06-08 11:28:27,985 epoch 19 - iter 89/891 - loss 0.12745529\n",
            "2019-06-08 11:28:47,831 epoch 19 - iter 178/891 - loss 0.11253529\n",
            "2019-06-08 11:29:09,832 epoch 19 - iter 267/891 - loss 0.10984872\n",
            "2019-06-08 11:29:28,878 epoch 19 - iter 356/891 - loss 0.10817668\n",
            "2019-06-08 11:29:47,960 epoch 19 - iter 445/891 - loss 0.10803773\n",
            "2019-06-08 11:30:06,888 epoch 19 - iter 534/891 - loss 0.10788213\n",
            "2019-06-08 11:30:27,898 epoch 19 - iter 623/891 - loss 0.10920929\n",
            "2019-06-08 11:30:46,917 epoch 19 - iter 712/891 - loss 0.11002281\n",
            "2019-06-08 11:31:05,650 epoch 19 - iter 801/891 - loss 0.11336606\n",
            "2019-06-08 11:31:26,660 epoch 19 - iter 890/891 - loss 0.11317397\n",
            "2019-06-08 11:31:27,662 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:31:27,665 EPOCH 19 done: loss 0.1132 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 11:31:49,924 DEV : loss 0.11782331019639969 - score 0.9138\n",
            "2019-06-08 11:33:21,473 TEST : loss 0.1043301522731781 - score 0.9224\n",
            "2019-06-08 11:33:26,985 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:33:28,173 epoch 20 - iter 0/891 - loss 0.01542780\n",
            "2019-06-08 11:33:51,089 epoch 20 - iter 89/891 - loss 0.10939122\n",
            "2019-06-08 11:34:10,680 epoch 20 - iter 178/891 - loss 0.11114595\n",
            "2019-06-08 11:34:29,863 epoch 20 - iter 267/891 - loss 0.11004599\n",
            "2019-06-08 11:34:49,384 epoch 20 - iter 356/891 - loss 0.11007357\n",
            "2019-06-08 11:35:10,441 epoch 20 - iter 445/891 - loss 0.11159273\n",
            "2019-06-08 11:35:29,356 epoch 20 - iter 534/891 - loss 0.10911684\n",
            "2019-06-08 11:35:48,127 epoch 20 - iter 623/891 - loss 0.11031778\n",
            "2019-06-08 11:36:09,107 epoch 20 - iter 712/891 - loss 0.11228809\n",
            "2019-06-08 11:36:28,097 epoch 20 - iter 801/891 - loss 0.11356614\n",
            "2019-06-08 11:36:47,057 epoch 20 - iter 890/891 - loss 0.11273255\n",
            "2019-06-08 11:36:48,042 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:36:48,044 EPOCH 20 done: loss 0.1127 - lr 0.1000 - bad epochs 1\n",
            "2019-06-08 11:37:10,254 DEV : loss 0.13291236758232117 - score 0.9158\n",
            "2019-06-08 11:38:43,966 TEST : loss 0.10942913591861725 - score 0.9165\n",
            "2019-06-08 11:38:49,433 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:38:50,579 epoch 21 - iter 0/891 - loss 0.05016890\n",
            "2019-06-08 11:39:10,811 epoch 21 - iter 89/891 - loss 0.11785455\n",
            "2019-06-08 11:39:30,692 epoch 21 - iter 178/891 - loss 0.11209429\n",
            "2019-06-08 11:39:53,226 epoch 21 - iter 267/891 - loss 0.11202924\n",
            "2019-06-08 11:40:12,497 epoch 21 - iter 356/891 - loss 0.11071482\n",
            "2019-06-08 11:40:31,609 epoch 21 - iter 445/891 - loss 0.11317072\n",
            "2019-06-08 11:40:53,046 epoch 21 - iter 534/891 - loss 0.11246424\n",
            "2019-06-08 11:41:12,528 epoch 21 - iter 623/891 - loss 0.11084174\n",
            "2019-06-08 11:41:31,817 epoch 21 - iter 712/891 - loss 0.11130893\n",
            "2019-06-08 11:41:50,846 epoch 21 - iter 801/891 - loss 0.11266629\n",
            "2019-06-08 11:42:12,135 epoch 21 - iter 890/891 - loss 0.11148020\n",
            "2019-06-08 11:42:13,109 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:42:13,110 EPOCH 21 done: loss 0.1115 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 11:42:35,581 DEV : loss 0.11760329455137253 - score 0.9186\n",
            "2019-06-08 11:44:08,304 TEST : loss 0.1118834912776947 - score 0.9063\n",
            "2019-06-08 11:44:13,854 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:44:15,025 epoch 22 - iter 0/891 - loss 0.32875091\n",
            "2019-06-08 11:44:37,918 epoch 22 - iter 89/891 - loss 0.09506920\n",
            "2019-06-08 11:44:57,628 epoch 22 - iter 178/891 - loss 0.10114996\n",
            "2019-06-08 11:45:17,129 epoch 22 - iter 267/891 - loss 0.10273879\n",
            "2019-06-08 11:45:38,603 epoch 22 - iter 356/891 - loss 0.10064664\n",
            "2019-06-08 11:45:57,693 epoch 22 - iter 445/891 - loss 0.10391040\n",
            "2019-06-08 11:46:17,100 epoch 22 - iter 534/891 - loss 0.10269792\n",
            "2019-06-08 11:46:36,347 epoch 22 - iter 623/891 - loss 0.10350276\n",
            "2019-06-08 11:46:57,594 epoch 22 - iter 712/891 - loss 0.10511170\n",
            "2019-06-08 11:47:16,618 epoch 22 - iter 801/891 - loss 0.10770239\n",
            "2019-06-08 11:47:35,778 epoch 22 - iter 890/891 - loss 0.10832544\n",
            "2019-06-08 11:47:36,757 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:47:36,762 EPOCH 22 done: loss 0.1083 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 11:48:02,167 DEV : loss 0.13181856274604797 - score 0.9105\n",
            "2019-06-08 11:49:35,258 TEST : loss 0.11158296465873718 - score 0.9209\n",
            "2019-06-08 11:49:35,265 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:49:36,265 epoch 23 - iter 0/891 - loss 0.01384901\n",
            "2019-06-08 11:49:56,099 epoch 23 - iter 89/891 - loss 0.10231347\n",
            "2019-06-08 11:50:18,486 epoch 23 - iter 178/891 - loss 0.11166494\n",
            "2019-06-08 11:50:38,333 epoch 23 - iter 267/891 - loss 0.10650014\n",
            "2019-06-08 11:50:57,354 epoch 23 - iter 356/891 - loss 0.10539869\n",
            "2019-06-08 11:51:16,597 epoch 23 - iter 445/891 - loss 0.10449348\n",
            "2019-06-08 11:51:38,292 epoch 23 - iter 534/891 - loss 0.10368858\n",
            "2019-06-08 11:51:57,487 epoch 23 - iter 623/891 - loss 0.10457941\n",
            "2019-06-08 11:52:16,854 epoch 23 - iter 712/891 - loss 0.10543371\n",
            "2019-06-08 11:52:38,257 epoch 23 - iter 801/891 - loss 0.10478642\n",
            "2019-06-08 11:52:57,582 epoch 23 - iter 890/891 - loss 0.10377916\n",
            "2019-06-08 11:52:58,546 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:52:58,548 EPOCH 23 done: loss 0.1038 - lr 0.1000 - bad epochs 1\n",
            "2019-06-08 11:53:21,134 DEV : loss 0.11841671168804169 - score 0.9154\n",
            "2019-06-08 11:54:54,399 TEST : loss 0.10546764731407166 - score 0.9154\n",
            "2019-06-08 11:54:54,405 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:54:55,439 epoch 24 - iter 0/891 - loss 0.06155847\n",
            "2019-06-08 11:55:18,202 epoch 24 - iter 89/891 - loss 0.09361377\n",
            "2019-06-08 11:55:37,933 epoch 24 - iter 178/891 - loss 0.09633428\n",
            "2019-06-08 11:55:57,703 epoch 24 - iter 267/891 - loss 0.10782716\n",
            "2019-06-08 11:56:19,341 epoch 24 - iter 356/891 - loss 0.10404603\n",
            "2019-06-08 11:56:38,760 epoch 24 - iter 445/891 - loss 0.10219341\n",
            "2019-06-08 11:56:58,230 epoch 24 - iter 534/891 - loss 0.10004430\n",
            "2019-06-08 11:57:17,299 epoch 24 - iter 623/891 - loss 0.10018367\n",
            "2019-06-08 11:57:38,688 epoch 24 - iter 712/891 - loss 0.10178414\n",
            "2019-06-08 11:57:57,838 epoch 24 - iter 801/891 - loss 0.10505493\n",
            "2019-06-08 11:58:16,820 epoch 24 - iter 890/891 - loss 0.10630625\n",
            "2019-06-08 11:58:17,838 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 11:58:17,842 EPOCH 24 done: loss 0.1063 - lr 0.1000 - bad epochs 2\n",
            "2019-06-08 11:58:43,522 DEV : loss 0.12143164873123169 - score 0.9112\n",
            "2019-06-08 12:00:16,792 TEST : loss 0.10613858699798584 - score 0.9203\n",
            "2019-06-08 12:00:16,798 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:00:17,845 epoch 25 - iter 0/891 - loss 0.10464317\n",
            "2019-06-08 12:00:38,044 epoch 25 - iter 89/891 - loss 0.11377072\n",
            "2019-06-08 12:00:58,045 epoch 25 - iter 178/891 - loss 0.10918714\n",
            "2019-06-08 12:01:20,476 epoch 25 - iter 267/891 - loss 0.10841730\n",
            "2019-06-08 12:01:39,800 epoch 25 - iter 356/891 - loss 0.10407364\n",
            "2019-06-08 12:01:59,258 epoch 25 - iter 445/891 - loss 0.10465046\n",
            "2019-06-08 12:02:20,716 epoch 25 - iter 534/891 - loss 0.10395326\n",
            "2019-06-08 12:02:40,046 epoch 25 - iter 623/891 - loss 0.10310646\n",
            "2019-06-08 12:02:59,106 epoch 25 - iter 712/891 - loss 0.10429969\n",
            "2019-06-08 12:03:20,555 epoch 25 - iter 801/891 - loss 0.10591739\n",
            "2019-06-08 12:03:39,820 epoch 25 - iter 890/891 - loss 0.10499548\n",
            "2019-06-08 12:03:40,818 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:03:40,819 EPOCH 25 done: loss 0.1050 - lr 0.1000 - bad epochs 3\n",
            "2019-06-08 12:04:03,315 DEV : loss 0.11751095950603485 - score 0.9221\n",
            "2019-06-08 12:05:36,146 TEST : loss 0.10530214011669159 - score 0.9205\n",
            "2019-06-08 12:05:41,640 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:05:42,810 epoch 26 - iter 0/891 - loss 0.12144506\n",
            "2019-06-08 12:06:06,209 epoch 26 - iter 89/891 - loss 0.09541556\n",
            "2019-06-08 12:06:25,823 epoch 26 - iter 178/891 - loss 0.09756733\n",
            "2019-06-08 12:06:45,288 epoch 26 - iter 267/891 - loss 0.10136187\n",
            "2019-06-08 12:07:07,151 epoch 26 - iter 356/891 - loss 0.09907031\n",
            "2019-06-08 12:07:26,120 epoch 26 - iter 445/891 - loss 0.09978866\n",
            "2019-06-08 12:07:45,205 epoch 26 - iter 534/891 - loss 0.10040553\n",
            "2019-06-08 12:08:06,474 epoch 26 - iter 623/891 - loss 0.10232123\n",
            "2019-06-08 12:08:25,674 epoch 26 - iter 712/891 - loss 0.10466576\n",
            "2019-06-08 12:08:44,962 epoch 26 - iter 801/891 - loss 0.10483528\n",
            "2019-06-08 12:09:04,111 epoch 26 - iter 890/891 - loss 0.10529301\n",
            "2019-06-08 12:09:05,117 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:09:05,118 EPOCH 26 done: loss 0.1053 - lr 0.1000 - bad epochs 0\n",
            "2019-06-08 12:09:30,695 DEV : loss 0.12639912962913513 - score 0.9172\n",
            "2019-06-08 12:11:03,858 TEST : loss 0.11508350074291229 - score 0.9157\n",
            "2019-06-08 12:11:03,864 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:11:04,885 epoch 27 - iter 0/891 - loss 0.27058658\n",
            "2019-06-08 12:11:24,673 epoch 27 - iter 89/891 - loss 0.09956074\n",
            "2019-06-08 12:11:47,344 epoch 27 - iter 178/891 - loss 0.09920264\n",
            "2019-06-08 12:12:07,022 epoch 27 - iter 267/891 - loss 0.10213415\n",
            "2019-06-08 12:12:26,592 epoch 27 - iter 356/891 - loss 0.10017498\n",
            "2019-06-08 12:12:47,859 epoch 27 - iter 445/891 - loss 0.10129434\n",
            "2019-06-08 12:13:07,146 epoch 27 - iter 534/891 - loss 0.10194263\n",
            "2019-06-08 12:13:26,287 epoch 27 - iter 623/891 - loss 0.10119649\n",
            "2019-06-08 12:13:45,499 epoch 27 - iter 712/891 - loss 0.10089442\n",
            "2019-06-08 12:14:06,728 epoch 27 - iter 801/891 - loss 0.10207359\n",
            "2019-06-08 12:14:25,984 epoch 27 - iter 890/891 - loss 0.10200674\n",
            "2019-06-08 12:14:26,975 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:14:26,980 EPOCH 27 done: loss 0.1020 - lr 0.1000 - bad epochs 1\n",
            "2019-06-08 12:14:49,551 DEV : loss 0.11740759015083313 - score 0.9146\n",
            "2019-06-08 12:16:24,623 TEST : loss 0.11211871355772018 - score 0.9164\n",
            "2019-06-08 12:16:24,630 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:16:25,619 epoch 28 - iter 0/891 - loss 0.10681807\n",
            "2019-06-08 12:16:45,641 epoch 28 - iter 89/891 - loss 0.10909914\n",
            "2019-06-08 12:17:05,700 epoch 28 - iter 178/891 - loss 0.10248153\n",
            "2019-06-08 12:17:28,070 epoch 28 - iter 267/891 - loss 0.10323229\n",
            "2019-06-08 12:17:47,340 epoch 28 - iter 356/891 - loss 0.10479787\n",
            "2019-06-08 12:18:06,623 epoch 28 - iter 445/891 - loss 0.10398455\n",
            "2019-06-08 12:18:25,800 epoch 28 - iter 534/891 - loss 0.10382514\n",
            "2019-06-08 12:18:47,163 epoch 28 - iter 623/891 - loss 0.10095266\n",
            "2019-06-08 12:19:06,385 epoch 28 - iter 712/891 - loss 0.10171384\n",
            "2019-06-08 12:19:25,388 epoch 28 - iter 801/891 - loss 0.09976336\n",
            "2019-06-08 12:19:46,274 epoch 28 - iter 890/891 - loss 0.09776604\n",
            "2019-06-08 12:19:47,237 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:19:47,239 EPOCH 28 done: loss 0.0978 - lr 0.1000 - bad epochs 2\n",
            "2019-06-08 12:20:09,279 DEV : loss 0.12458417564630508 - score 0.9191\n",
            "2019-06-08 12:21:40,568 TEST : loss 0.11142102628946304 - score 0.9101\n",
            "2019-06-08 12:21:40,574 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:21:41,567 epoch 29 - iter 0/891 - loss 0.00389305\n",
            "2019-06-08 12:22:03,783 epoch 29 - iter 89/891 - loss 0.09073648\n",
            "2019-06-08 12:22:23,040 epoch 29 - iter 178/891 - loss 0.09494538\n",
            "2019-06-08 12:22:42,534 epoch 29 - iter 267/891 - loss 0.09323776\n",
            "2019-06-08 12:23:01,443 epoch 29 - iter 356/891 - loss 0.09433734\n",
            "2019-06-08 12:23:22,523 epoch 29 - iter 445/891 - loss 0.09539439\n",
            "2019-06-08 12:23:41,648 epoch 29 - iter 534/891 - loss 0.09574865\n",
            "2019-06-08 12:24:00,695 epoch 29 - iter 623/891 - loss 0.09370818\n",
            "2019-06-08 12:24:21,750 epoch 29 - iter 712/891 - loss 0.09538117\n",
            "2019-06-08 12:24:40,674 epoch 29 - iter 801/891 - loss 0.09655511\n",
            "2019-06-08 12:24:59,397 epoch 29 - iter 890/891 - loss 0.09701022\n",
            "2019-06-08 12:25:00,370 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:25:00,372 EPOCH 29 done: loss 0.0970 - lr 0.1000 - bad epochs 3\n",
            "2019-06-08 12:25:22,338 DEV : loss 0.11732298880815506 - score 0.9195\n",
            "2019-06-08 12:26:55,502 TEST : loss 0.10358047485351562 - score 0.9169\n",
            "Epoch    28: reducing learning rate of group 0 to 5.0000e-02.\n",
            "2019-06-08 12:26:55,509 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:26:56,478 epoch 30 - iter 0/891 - loss 0.20492703\n",
            "2019-06-08 12:27:15,928 epoch 30 - iter 89/891 - loss 0.07914629\n",
            "2019-06-08 12:27:35,528 epoch 30 - iter 178/891 - loss 0.07772619\n",
            "2019-06-08 12:27:57,484 epoch 30 - iter 267/891 - loss 0.08181691\n",
            "2019-06-08 12:28:16,558 epoch 30 - iter 356/891 - loss 0.08573179\n",
            "2019-06-08 12:28:35,530 epoch 30 - iter 445/891 - loss 0.08506625\n",
            "2019-06-08 12:28:54,174 epoch 30 - iter 534/891 - loss 0.08625074\n",
            "2019-06-08 12:29:15,062 epoch 30 - iter 623/891 - loss 0.08730124\n",
            "2019-06-08 12:29:34,022 epoch 30 - iter 712/891 - loss 0.08802239\n",
            "2019-06-08 12:29:52,961 epoch 30 - iter 801/891 - loss 0.08888154\n",
            "2019-06-08 12:30:13,751 epoch 30 - iter 890/891 - loss 0.09182137\n",
            "2019-06-08 12:30:14,688 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:30:14,689 EPOCH 30 done: loss 0.0918 - lr 0.0500 - bad epochs 0\n",
            "2019-06-08 12:30:36,714 DEV : loss 0.11809007078409195 - score 0.9172\n",
            "2019-06-08 12:32:06,561 TEST : loss 0.10550838708877563 - score 0.9202\n",
            "2019-06-08 12:32:06,567 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:32:07,520 epoch 31 - iter 0/891 - loss 0.21934986\n",
            "2019-06-08 12:32:29,394 epoch 31 - iter 89/891 - loss 0.10444520\n",
            "2019-06-08 12:32:48,678 epoch 31 - iter 178/891 - loss 0.09437502\n",
            "2019-06-08 12:33:07,952 epoch 31 - iter 267/891 - loss 0.09047250\n",
            "2019-06-08 12:33:27,029 epoch 31 - iter 356/891 - loss 0.09176773\n",
            "2019-06-08 12:33:48,315 epoch 31 - iter 445/891 - loss 0.09305559\n",
            "2019-06-08 12:34:07,109 epoch 31 - iter 534/891 - loss 0.09281293\n",
            "2019-06-08 12:34:25,820 epoch 31 - iter 623/891 - loss 0.09235657\n",
            "2019-06-08 12:34:46,756 epoch 31 - iter 712/891 - loss 0.09189218\n",
            "2019-06-08 12:35:05,553 epoch 31 - iter 801/891 - loss 0.09150289\n",
            "2019-06-08 12:35:24,078 epoch 31 - iter 890/891 - loss 0.09126956\n",
            "2019-06-08 12:35:25,042 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:35:25,046 EPOCH 31 done: loss 0.0913 - lr 0.0500 - bad epochs 1\n",
            "2019-06-08 12:35:46,914 DEV : loss 0.11478347331285477 - score 0.9184\n",
            "2019-06-08 12:37:18,938 TEST : loss 0.10641957819461823 - score 0.9226\n",
            "2019-06-08 12:37:18,944 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:37:19,923 epoch 32 - iter 0/891 - loss 0.00508475\n",
            "2019-06-08 12:37:39,369 epoch 32 - iter 89/891 - loss 0.08110007\n",
            "2019-06-08 12:37:58,922 epoch 32 - iter 178/891 - loss 0.08480084\n",
            "2019-06-08 12:38:20,597 epoch 32 - iter 267/891 - loss 0.08870019\n",
            "2019-06-08 12:38:39,709 epoch 32 - iter 356/891 - loss 0.08591763\n",
            "2019-06-08 12:38:58,522 epoch 32 - iter 445/891 - loss 0.08787389\n",
            "2019-06-08 12:39:19,091 epoch 32 - iter 534/891 - loss 0.08508723\n",
            "2019-06-08 12:39:37,905 epoch 32 - iter 623/891 - loss 0.08422441\n",
            "2019-06-08 12:39:56,697 epoch 32 - iter 712/891 - loss 0.08520900\n",
            "2019-06-08 12:40:15,386 epoch 32 - iter 801/891 - loss 0.08718101\n",
            "2019-06-08 12:40:36,076 epoch 32 - iter 890/891 - loss 0.08783100\n",
            "2019-06-08 12:40:37,030 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:40:37,034 EPOCH 32 done: loss 0.0878 - lr 0.0500 - bad epochs 2\n",
            "2019-06-08 12:40:58,926 DEV : loss 0.1142662763595581 - score 0.9158\n",
            "2019-06-08 12:42:28,803 TEST : loss 0.10521723330020905 - score 0.9156\n",
            "2019-06-08 12:42:28,809 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:42:29,790 epoch 33 - iter 0/891 - loss 0.37599677\n",
            "2019-06-08 12:42:52,035 epoch 33 - iter 89/891 - loss 0.10236785\n",
            "2019-06-08 12:43:11,120 epoch 33 - iter 178/891 - loss 0.09275241\n",
            "2019-06-08 12:43:30,329 epoch 33 - iter 267/891 - loss 0.08518822\n",
            "2019-06-08 12:43:49,196 epoch 33 - iter 356/891 - loss 0.08918226\n",
            "2019-06-08 12:44:10,131 epoch 33 - iter 445/891 - loss 0.08999276\n",
            "2019-06-08 12:44:28,809 epoch 33 - iter 534/891 - loss 0.08970412\n",
            "2019-06-08 12:44:47,514 epoch 33 - iter 623/891 - loss 0.08713253\n",
            "2019-06-08 12:45:08,452 epoch 33 - iter 712/891 - loss 0.08694946\n",
            "2019-06-08 12:45:27,316 epoch 33 - iter 801/891 - loss 0.08750542\n",
            "2019-06-08 12:45:45,813 epoch 33 - iter 890/891 - loss 0.08947272\n",
            "2019-06-08 12:45:46,776 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:45:46,780 EPOCH 33 done: loss 0.0895 - lr 0.0500 - bad epochs 3\n",
            "2019-06-08 12:46:11,250 DEV : loss 0.11238697171211243 - score 0.9189\n",
            "2019-06-08 12:47:41,140 TEST : loss 0.10484827309846878 - score 0.921\n",
            "Epoch    32: reducing learning rate of group 0 to 2.5000e-02.\n",
            "2019-06-08 12:47:41,147 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:47:42,130 epoch 34 - iter 0/891 - loss 0.17537963\n",
            "2019-06-08 12:48:01,670 epoch 34 - iter 89/891 - loss 0.07759411\n",
            "2019-06-08 12:48:21,118 epoch 34 - iter 178/891 - loss 0.08279375\n",
            "2019-06-08 12:48:42,690 epoch 34 - iter 267/891 - loss 0.08750803\n",
            "2019-06-08 12:49:01,474 epoch 34 - iter 356/891 - loss 0.08496341\n",
            "2019-06-08 12:49:20,438 epoch 34 - iter 445/891 - loss 0.08242767\n",
            "2019-06-08 12:49:41,160 epoch 34 - iter 534/891 - loss 0.08346737\n",
            "2019-06-08 12:49:59,925 epoch 34 - iter 623/891 - loss 0.08242914\n",
            "2019-06-08 12:50:18,772 epoch 34 - iter 712/891 - loss 0.08272958\n",
            "2019-06-08 12:50:37,740 epoch 34 - iter 801/891 - loss 0.08405521\n",
            "2019-06-08 12:50:58,251 epoch 34 - iter 890/891 - loss 0.08470050\n",
            "2019-06-08 12:50:59,158 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:50:59,162 EPOCH 34 done: loss 0.0847 - lr 0.0250 - bad epochs 0\n",
            "2019-06-08 12:51:20,866 DEV : loss 0.11296360939741135 - score 0.9199\n",
            "2019-06-08 12:52:50,650 TEST : loss 0.10572067648172379 - score 0.9227\n",
            "2019-06-08 12:52:50,656 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:52:51,615 epoch 35 - iter 0/891 - loss 0.00775790\n",
            "2019-06-08 12:53:13,653 epoch 35 - iter 89/891 - loss 0.08194186\n",
            "2019-06-08 12:53:33,003 epoch 35 - iter 178/891 - loss 0.07881959\n",
            "2019-06-08 12:53:52,092 epoch 35 - iter 267/891 - loss 0.08284488\n",
            "2019-06-08 12:54:13,191 epoch 35 - iter 356/891 - loss 0.08052642\n",
            "2019-06-08 12:54:32,021 epoch 35 - iter 445/891 - loss 0.08059006\n",
            "2019-06-08 12:54:50,640 epoch 35 - iter 534/891 - loss 0.08114010\n",
            "2019-06-08 12:55:09,229 epoch 35 - iter 623/891 - loss 0.08082099\n",
            "2019-06-08 12:55:30,106 epoch 35 - iter 712/891 - loss 0.08338610\n",
            "2019-06-08 12:55:48,711 epoch 35 - iter 801/891 - loss 0.08308617\n",
            "2019-06-08 12:56:07,429 epoch 35 - iter 890/891 - loss 0.08268542\n",
            "2019-06-08 12:56:08,375 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:56:08,376 EPOCH 35 done: loss 0.0827 - lr 0.0250 - bad epochs 1\n",
            "2019-06-08 12:56:32,842 DEV : loss 0.1129690408706665 - score 0.9212\n",
            "2019-06-08 12:58:02,441 TEST : loss 0.10508476197719574 - score 0.9203\n",
            "2019-06-08 12:58:02,447 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 12:58:03,414 epoch 36 - iter 0/891 - loss 0.03521894\n",
            "2019-06-08 12:58:22,787 epoch 36 - iter 89/891 - loss 0.07738119\n",
            "2019-06-08 12:58:42,237 epoch 36 - iter 178/891 - loss 0.07494254\n",
            "2019-06-08 12:59:04,068 epoch 36 - iter 267/891 - loss 0.07685461\n",
            "2019-06-08 12:59:22,989 epoch 36 - iter 356/891 - loss 0.07323265\n",
            "2019-06-08 12:59:41,704 epoch 36 - iter 445/891 - loss 0.07359495\n",
            "2019-06-08 13:00:02,664 epoch 36 - iter 534/891 - loss 0.07179091\n",
            "2019-06-08 13:00:21,314 epoch 36 - iter 623/891 - loss 0.07702146\n",
            "2019-06-08 13:00:39,973 epoch 36 - iter 712/891 - loss 0.07891248\n",
            "2019-06-08 13:01:00,532 epoch 36 - iter 801/891 - loss 0.07956417\n",
            "2019-06-08 13:01:19,146 epoch 36 - iter 890/891 - loss 0.08082949\n",
            "2019-06-08 13:01:20,115 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:01:20,117 EPOCH 36 done: loss 0.0808 - lr 0.0250 - bad epochs 2\n",
            "2019-06-08 13:01:41,692 DEV : loss 0.11373211443424225 - score 0.9238\n",
            "2019-06-08 13:03:10,964 TEST : loss 0.10806320607662201 - score 0.9191\n",
            "2019-06-08 13:03:16,239 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:03:17,395 epoch 37 - iter 0/891 - loss 0.04149321\n",
            "2019-06-08 13:03:39,741 epoch 37 - iter 89/891 - loss 0.07655771\n",
            "2019-06-08 13:03:59,287 epoch 37 - iter 178/891 - loss 0.08959999\n",
            "2019-06-08 13:04:18,252 epoch 37 - iter 267/891 - loss 0.08961522\n",
            "2019-06-08 13:04:38,899 epoch 37 - iter 356/891 - loss 0.09154552\n",
            "2019-06-08 13:04:57,668 epoch 37 - iter 445/891 - loss 0.08948885\n",
            "2019-06-08 13:05:16,399 epoch 37 - iter 534/891 - loss 0.08718154\n",
            "2019-06-08 13:05:37,182 epoch 37 - iter 623/891 - loss 0.08471852\n",
            "2019-06-08 13:05:55,692 epoch 37 - iter 712/891 - loss 0.08551546\n",
            "2019-06-08 13:06:14,307 epoch 37 - iter 801/891 - loss 0.08377115\n",
            "2019-06-08 13:06:32,916 epoch 37 - iter 890/891 - loss 0.08334604\n",
            "2019-06-08 13:06:33,881 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:06:33,887 EPOCH 37 done: loss 0.0833 - lr 0.0250 - bad epochs 0\n",
            "2019-06-08 13:06:58,265 DEV : loss 0.11066058278083801 - score 0.9219\n",
            "2019-06-08 13:08:27,430 TEST : loss 0.10334230214357376 - score 0.9199\n",
            "2019-06-08 13:08:27,438 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:08:28,404 epoch 38 - iter 0/891 - loss 0.02211407\n",
            "2019-06-08 13:08:47,665 epoch 38 - iter 89/891 - loss 0.09184755\n",
            "2019-06-08 13:09:09,613 epoch 38 - iter 178/891 - loss 0.08403895\n",
            "2019-06-08 13:09:28,612 epoch 38 - iter 267/891 - loss 0.08442770\n",
            "2019-06-08 13:09:47,697 epoch 38 - iter 356/891 - loss 0.08185801\n",
            "2019-06-08 13:10:06,448 epoch 38 - iter 445/891 - loss 0.08556653\n",
            "2019-06-08 13:10:27,148 epoch 38 - iter 534/891 - loss 0.08473555\n",
            "2019-06-08 13:10:45,626 epoch 38 - iter 623/891 - loss 0.08357629\n",
            "2019-06-08 13:11:04,222 epoch 38 - iter 712/891 - loss 0.08276479\n",
            "2019-06-08 13:11:24,712 epoch 38 - iter 801/891 - loss 0.08053390\n",
            "2019-06-08 13:11:43,254 epoch 38 - iter 890/891 - loss 0.08040616\n",
            "2019-06-08 13:11:44,182 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:11:44,186 EPOCH 38 done: loss 0.0804 - lr 0.0250 - bad epochs 1\n",
            "2019-06-08 13:12:05,789 DEV : loss 0.11529231816530228 - score 0.9279\n",
            "2019-06-08 13:13:36,785 TEST : loss 0.10520212352275848 - score 0.9164\n",
            "2019-06-08 13:13:42,147 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:13:43,274 epoch 39 - iter 0/891 - loss 0.00701410\n",
            "2019-06-08 13:14:03,106 epoch 39 - iter 89/891 - loss 0.06184378\n",
            "2019-06-08 13:14:22,461 epoch 39 - iter 178/891 - loss 0.07333736\n",
            "2019-06-08 13:14:41,803 epoch 39 - iter 267/891 - loss 0.07684609\n",
            "2019-06-08 13:15:02,722 epoch 39 - iter 356/891 - loss 0.07735114\n",
            "2019-06-08 13:15:21,413 epoch 39 - iter 445/891 - loss 0.07490255\n",
            "2019-06-08 13:15:40,103 epoch 39 - iter 534/891 - loss 0.07737777\n",
            "2019-06-08 13:16:00,743 epoch 39 - iter 623/891 - loss 0.07667587\n",
            "2019-06-08 13:16:19,493 epoch 39 - iter 712/891 - loss 0.07873324\n",
            "2019-06-08 13:16:38,195 epoch 39 - iter 801/891 - loss 0.08104019\n",
            "2019-06-08 13:16:56,569 epoch 39 - iter 890/891 - loss 0.08152454\n",
            "2019-06-08 13:16:57,499 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:16:57,501 EPOCH 39 done: loss 0.0815 - lr 0.0250 - bad epochs 0\n",
            "2019-06-08 13:17:21,905 DEV : loss 0.11013543605804443 - score 0.9225\n",
            "2019-06-08 13:18:51,067 TEST : loss 0.10646426677703857 - score 0.9178\n",
            "2019-06-08 13:18:51,076 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:18:52,046 epoch 40 - iter 0/891 - loss 0.00260460\n",
            "2019-06-08 13:19:11,296 epoch 40 - iter 89/891 - loss 0.07492759\n",
            "2019-06-08 13:19:33,192 epoch 40 - iter 178/891 - loss 0.08621763\n",
            "2019-06-08 13:19:52,048 epoch 40 - iter 267/891 - loss 0.08313671\n",
            "2019-06-08 13:20:10,793 epoch 40 - iter 356/891 - loss 0.08510776\n",
            "2019-06-08 13:20:31,611 epoch 40 - iter 445/891 - loss 0.08142868\n",
            "2019-06-08 13:20:50,218 epoch 40 - iter 534/891 - loss 0.08046107\n",
            "2019-06-08 13:21:08,864 epoch 40 - iter 623/891 - loss 0.08241632\n",
            "2019-06-08 13:21:27,307 epoch 40 - iter 712/891 - loss 0.08352957\n",
            "2019-06-08 13:21:48,126 epoch 40 - iter 801/891 - loss 0.08329420\n",
            "2019-06-08 13:22:06,638 epoch 40 - iter 890/891 - loss 0.08363274\n",
            "2019-06-08 13:22:07,552 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:22:07,554 EPOCH 40 done: loss 0.0836 - lr 0.0250 - bad epochs 1\n",
            "2019-06-08 13:22:29,083 DEV : loss 0.11104332655668259 - score 0.9248\n",
            "2019-06-08 13:24:00,101 TEST : loss 0.10461002588272095 - score 0.9183\n",
            "2019-06-08 13:24:00,109 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:24:01,064 epoch 41 - iter 0/891 - loss 0.25133574\n",
            "2019-06-08 13:24:20,466 epoch 41 - iter 89/891 - loss 0.08438589\n",
            "2019-06-08 13:24:39,755 epoch 41 - iter 178/891 - loss 0.07321147\n",
            "2019-06-08 13:25:01,160 epoch 41 - iter 267/891 - loss 0.07642076\n",
            "2019-06-08 13:25:19,746 epoch 41 - iter 356/891 - loss 0.07686922\n",
            "2019-06-08 13:25:38,566 epoch 41 - iter 445/891 - loss 0.07904101\n",
            "2019-06-08 13:25:57,090 epoch 41 - iter 534/891 - loss 0.08068448\n",
            "2019-06-08 13:26:17,581 epoch 41 - iter 623/891 - loss 0.08125199\n",
            "2019-06-08 13:26:36,408 epoch 41 - iter 712/891 - loss 0.08111023\n",
            "2019-06-08 13:26:55,047 epoch 41 - iter 801/891 - loss 0.08078220\n",
            "2019-06-08 13:27:15,657 epoch 41 - iter 890/891 - loss 0.08096445\n",
            "2019-06-08 13:27:16,576 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:27:16,578 EPOCH 41 done: loss 0.0810 - lr 0.0250 - bad epochs 2\n",
            "2019-06-08 13:27:38,230 DEV : loss 0.10983305424451828 - score 0.927\n",
            "2019-06-08 13:29:07,411 TEST : loss 0.10466562211513519 - score 0.9189\n",
            "2019-06-08 13:29:07,421 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:29:08,386 epoch 42 - iter 0/891 - loss 0.09157079\n",
            "2019-06-08 13:29:27,709 epoch 42 - iter 89/891 - loss 0.08195602\n",
            "2019-06-08 13:29:49,312 epoch 42 - iter 178/891 - loss 0.07992750\n",
            "2019-06-08 13:30:08,199 epoch 42 - iter 267/891 - loss 0.07897971\n",
            "2019-06-08 13:30:27,110 epoch 42 - iter 356/891 - loss 0.07771165\n",
            "2019-06-08 13:30:48,001 epoch 42 - iter 445/891 - loss 0.08093042\n",
            "2019-06-08 13:31:06,851 epoch 42 - iter 534/891 - loss 0.08029982\n",
            "2019-06-08 13:31:25,460 epoch 42 - iter 623/891 - loss 0.08082723\n",
            "2019-06-08 13:31:45,994 epoch 42 - iter 712/891 - loss 0.08034356\n",
            "2019-06-08 13:32:04,660 epoch 42 - iter 801/891 - loss 0.07953613\n",
            "2019-06-08 13:32:23,225 epoch 42 - iter 890/891 - loss 0.08011903\n",
            "2019-06-08 13:32:24,138 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:32:24,139 EPOCH 42 done: loss 0.0801 - lr 0.0250 - bad epochs 3\n",
            "2019-06-08 13:32:45,804 DEV : loss 0.11352666467428207 - score 0.9252\n",
            "2019-06-08 13:34:16,994 TEST : loss 0.10552889853715897 - score 0.9206\n",
            "Epoch    41: reducing learning rate of group 0 to 1.2500e-02.\n",
            "2019-06-08 13:34:17,003 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:34:17,977 epoch 43 - iter 0/891 - loss 0.01515310\n",
            "2019-06-08 13:34:37,350 epoch 43 - iter 89/891 - loss 0.08830085\n",
            "2019-06-08 13:34:56,666 epoch 43 - iter 178/891 - loss 0.08556570\n",
            "2019-06-08 13:35:18,269 epoch 43 - iter 267/891 - loss 0.08676810\n",
            "2019-06-08 13:35:36,879 epoch 43 - iter 356/891 - loss 0.08263528\n",
            "2019-06-08 13:35:55,578 epoch 43 - iter 445/891 - loss 0.08176258\n",
            "2019-06-08 13:36:16,329 epoch 43 - iter 534/891 - loss 0.08040688\n",
            "2019-06-08 13:36:35,095 epoch 43 - iter 623/891 - loss 0.07982923\n",
            "2019-06-08 13:36:53,853 epoch 43 - iter 712/891 - loss 0.07976897\n",
            "2019-06-08 13:37:12,384 epoch 43 - iter 801/891 - loss 0.07978255\n",
            "2019-06-08 13:37:32,926 epoch 43 - iter 890/891 - loss 0.07879753\n",
            "2019-06-08 13:37:33,850 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:37:33,856 EPOCH 43 done: loss 0.0788 - lr 0.0125 - bad epochs 0\n",
            "2019-06-08 13:37:55,410 DEV : loss 0.11238335072994232 - score 0.9276\n",
            "2019-06-08 13:39:24,456 TEST : loss 0.10668718814849854 - score 0.9217\n",
            "2019-06-08 13:39:24,464 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:39:25,405 epoch 44 - iter 0/891 - loss 0.13361371\n",
            "2019-06-08 13:39:47,223 epoch 44 - iter 89/891 - loss 0.07387384\n",
            "2019-06-08 13:40:06,312 epoch 44 - iter 178/891 - loss 0.07445367\n",
            "2019-06-08 13:40:25,451 epoch 44 - iter 267/891 - loss 0.07656220\n",
            "2019-06-08 13:40:46,312 epoch 44 - iter 356/891 - loss 0.07626644\n",
            "2019-06-08 13:41:04,886 epoch 44 - iter 445/891 - loss 0.07500503\n",
            "2019-06-08 13:41:23,486 epoch 44 - iter 534/891 - loss 0.07478247\n",
            "2019-06-08 13:41:42,161 epoch 44 - iter 623/891 - loss 0.07491632\n",
            "2019-06-08 13:42:02,884 epoch 44 - iter 712/891 - loss 0.07513148\n",
            "2019-06-08 13:42:21,400 epoch 44 - iter 801/891 - loss 0.07669832\n",
            "2019-06-08 13:42:39,962 epoch 44 - iter 890/891 - loss 0.07807739\n",
            "2019-06-08 13:42:40,896 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:42:40,902 EPOCH 44 done: loss 0.0781 - lr 0.0125 - bad epochs 1\n",
            "2019-06-08 13:43:05,195 DEV : loss 0.11247844249010086 - score 0.9247\n",
            "2019-06-08 13:44:34,258 TEST : loss 0.10719745606184006 - score 0.9225\n",
            "2019-06-08 13:44:34,268 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:44:35,238 epoch 45 - iter 0/891 - loss 0.20238726\n",
            "2019-06-08 13:44:54,439 epoch 45 - iter 89/891 - loss 0.07691638\n",
            "2019-06-08 13:45:13,724 epoch 45 - iter 178/891 - loss 0.07975753\n",
            "2019-06-08 13:45:35,317 epoch 45 - iter 267/891 - loss 0.07662445\n",
            "2019-06-08 13:45:53,983 epoch 45 - iter 356/891 - loss 0.07749336\n",
            "2019-06-08 13:46:12,526 epoch 45 - iter 445/891 - loss 0.07508413\n",
            "2019-06-08 13:46:33,337 epoch 45 - iter 534/891 - loss 0.07616916\n",
            "2019-06-08 13:46:51,977 epoch 45 - iter 623/891 - loss 0.07732502\n",
            "2019-06-08 13:47:10,698 epoch 45 - iter 712/891 - loss 0.07811811\n",
            "2019-06-08 13:47:29,453 epoch 45 - iter 801/891 - loss 0.07937814\n",
            "2019-06-08 13:47:49,722 epoch 45 - iter 890/891 - loss 0.07846603\n",
            "2019-06-08 13:47:50,636 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:47:50,641 EPOCH 45 done: loss 0.0785 - lr 0.0125 - bad epochs 2\n",
            "2019-06-08 13:48:12,205 DEV : loss 0.11099618673324585 - score 0.9249\n",
            "2019-06-08 13:49:41,194 TEST : loss 0.10566896200180054 - score 0.9206\n",
            "2019-06-08 13:49:41,202 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:49:42,141 epoch 46 - iter 0/891 - loss 0.36787689\n",
            "2019-06-08 13:50:04,186 epoch 46 - iter 89/891 - loss 0.07506282\n",
            "2019-06-08 13:50:23,204 epoch 46 - iter 178/891 - loss 0.07477077\n",
            "2019-06-08 13:50:42,388 epoch 46 - iter 267/891 - loss 0.07784930\n",
            "2019-06-08 13:51:01,349 epoch 46 - iter 356/891 - loss 0.07728133\n",
            "2019-06-08 13:51:22,024 epoch 46 - iter 445/891 - loss 0.07890417\n",
            "2019-06-08 13:51:40,816 epoch 46 - iter 534/891 - loss 0.07670403\n",
            "2019-06-08 13:51:59,417 epoch 46 - iter 623/891 - loss 0.07809687\n",
            "2019-06-08 13:52:20,138 epoch 46 - iter 712/891 - loss 0.07818054\n",
            "2019-06-08 13:52:38,836 epoch 46 - iter 801/891 - loss 0.07854764\n",
            "2019-06-08 13:52:57,287 epoch 46 - iter 890/891 - loss 0.07865656\n",
            "2019-06-08 13:52:58,200 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:52:58,205 EPOCH 46 done: loss 0.0787 - lr 0.0125 - bad epochs 3\n",
            "2019-06-08 13:53:22,428 DEV : loss 0.11105301231145859 - score 0.9268\n",
            "2019-06-08 13:54:51,739 TEST : loss 0.10555931180715561 - score 0.922\n",
            "Epoch    45: reducing learning rate of group 0 to 6.2500e-03.\n",
            "2019-06-08 13:54:51,748 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:54:52,732 epoch 47 - iter 0/891 - loss 0.01609567\n",
            "2019-06-08 13:55:12,059 epoch 47 - iter 89/891 - loss 0.09288396\n",
            "2019-06-08 13:55:31,230 epoch 47 - iter 178/891 - loss 0.08746081\n",
            "2019-06-08 13:55:52,842 epoch 47 - iter 267/891 - loss 0.08431235\n",
            "2019-06-08 13:56:11,560 epoch 47 - iter 356/891 - loss 0.08639710\n",
            "2019-06-08 13:56:30,297 epoch 47 - iter 445/891 - loss 0.08511126\n",
            "2019-06-08 13:56:50,808 epoch 47 - iter 534/891 - loss 0.08494380\n",
            "2019-06-08 13:57:09,594 epoch 47 - iter 623/891 - loss 0.08400124\n",
            "2019-06-08 13:57:28,088 epoch 47 - iter 712/891 - loss 0.08200873\n",
            "2019-06-08 13:57:48,505 epoch 47 - iter 801/891 - loss 0.08124418\n",
            "2019-06-08 13:58:06,921 epoch 47 - iter 890/891 - loss 0.07991273\n",
            "2019-06-08 13:58:07,886 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:58:07,888 EPOCH 47 done: loss 0.0799 - lr 0.0063 - bad epochs 0\n",
            "2019-06-08 13:58:29,485 DEV : loss 0.11099354922771454 - score 0.9262\n",
            "2019-06-08 13:59:58,322 TEST : loss 0.10570038110017776 - score 0.9218\n",
            "2019-06-08 13:59:58,330 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 13:59:59,285 epoch 48 - iter 0/891 - loss 0.00813141\n",
            "2019-06-08 14:00:21,252 epoch 48 - iter 89/891 - loss 0.06315919\n",
            "2019-06-08 14:00:40,348 epoch 48 - iter 178/891 - loss 0.06859822\n",
            "2019-06-08 14:00:59,353 epoch 48 - iter 267/891 - loss 0.07508769\n",
            "2019-06-08 14:01:20,177 epoch 48 - iter 356/891 - loss 0.07487155\n",
            "2019-06-08 14:01:38,855 epoch 48 - iter 445/891 - loss 0.07804531\n",
            "2019-06-08 14:01:57,303 epoch 48 - iter 534/891 - loss 0.07869137\n",
            "2019-06-08 14:02:16,076 epoch 48 - iter 623/891 - loss 0.07841586\n",
            "2019-06-08 14:02:36,597 epoch 48 - iter 712/891 - loss 0.07970486\n",
            "2019-06-08 14:02:55,174 epoch 48 - iter 801/891 - loss 0.07868306\n",
            "2019-06-08 14:03:13,682 epoch 48 - iter 890/891 - loss 0.07717913\n",
            "2019-06-08 14:03:14,613 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:03:14,617 EPOCH 48 done: loss 0.0772 - lr 0.0063 - bad epochs 1\n",
            "2019-06-08 14:03:38,919 DEV : loss 0.1119149699807167 - score 0.9275\n",
            "2019-06-08 14:05:07,934 TEST : loss 0.1056138426065445 - score 0.919\n",
            "2019-06-08 14:05:07,943 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:05:08,919 epoch 49 - iter 0/891 - loss 0.08176382\n",
            "2019-06-08 14:05:27,955 epoch 49 - iter 89/891 - loss 0.07306557\n",
            "2019-06-08 14:05:49,616 epoch 49 - iter 178/891 - loss 0.07730907\n",
            "2019-06-08 14:06:08,634 epoch 49 - iter 267/891 - loss 0.07643263\n",
            "2019-06-08 14:06:27,271 epoch 49 - iter 356/891 - loss 0.07866518\n",
            "2019-06-08 14:06:45,974 epoch 49 - iter 445/891 - loss 0.07648692\n",
            "2019-06-08 14:07:06,644 epoch 49 - iter 534/891 - loss 0.07499586\n",
            "2019-06-08 14:07:25,306 epoch 49 - iter 623/891 - loss 0.07719572\n",
            "2019-06-08 14:07:43,898 epoch 49 - iter 712/891 - loss 0.07714584\n",
            "2019-06-08 14:08:04,322 epoch 49 - iter 801/891 - loss 0.07779671\n",
            "2019-06-08 14:08:22,715 epoch 49 - iter 890/891 - loss 0.07661366\n",
            "2019-06-08 14:08:23,636 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:08:23,638 EPOCH 49 done: loss 0.0766 - lr 0.0063 - bad epochs 2\n",
            "2019-06-08 14:08:45,110 DEV : loss 0.11178401857614517 - score 0.9271\n",
            "2019-06-08 14:10:13,901 TEST : loss 0.10533958673477173 - score 0.9212\n",
            "2019-06-08 14:10:13,908 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:10:14,857 epoch 50 - iter 0/891 - loss 0.26527083\n",
            "2019-06-08 14:10:36,843 epoch 50 - iter 89/891 - loss 0.06943781\n",
            "2019-06-08 14:10:55,853 epoch 50 - iter 178/891 - loss 0.07069202\n",
            "2019-06-08 14:11:14,802 epoch 50 - iter 267/891 - loss 0.07429691\n",
            "2019-06-08 14:11:35,656 epoch 50 - iter 356/891 - loss 0.07345664\n",
            "2019-06-08 14:11:54,371 epoch 50 - iter 445/891 - loss 0.07476183\n",
            "2019-06-08 14:12:12,991 epoch 50 - iter 534/891 - loss 0.07359188\n",
            "2019-06-08 14:12:33,462 epoch 50 - iter 623/891 - loss 0.07581831\n",
            "2019-06-08 14:12:52,063 epoch 50 - iter 712/891 - loss 0.07491222\n",
            "2019-06-08 14:13:10,594 epoch 50 - iter 801/891 - loss 0.07469278\n",
            "2019-06-08 14:13:29,012 epoch 50 - iter 890/891 - loss 0.07609700\n",
            "2019-06-08 14:13:29,931 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:13:29,932 EPOCH 50 done: loss 0.0761 - lr 0.0063 - bad epochs 3\n",
            "2019-06-08 14:13:54,219 DEV : loss 0.11040977388620377 - score 0.9275\n",
            "2019-06-08 14:15:22,711 TEST : loss 0.10524380207061768 - score 0.9194\n",
            "Epoch    49: reducing learning rate of group 0 to 3.1250e-03.\n",
            "2019-06-08 14:15:22,720 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:15:23,682 epoch 51 - iter 0/891 - loss 0.07608457\n",
            "2019-06-08 14:15:42,958 epoch 51 - iter 89/891 - loss 0.07681506\n",
            "2019-06-08 14:16:04,518 epoch 51 - iter 178/891 - loss 0.06977330\n",
            "2019-06-08 14:16:23,712 epoch 51 - iter 267/891 - loss 0.07066971\n",
            "2019-06-08 14:16:42,445 epoch 51 - iter 356/891 - loss 0.07496475\n",
            "2019-06-08 14:17:03,144 epoch 51 - iter 445/891 - loss 0.07251621\n",
            "2019-06-08 14:17:21,673 epoch 51 - iter 534/891 - loss 0.07327547\n",
            "2019-06-08 14:17:40,081 epoch 51 - iter 623/891 - loss 0.07362146\n",
            "2019-06-08 14:17:58,586 epoch 51 - iter 712/891 - loss 0.07553697\n",
            "2019-06-08 14:18:19,147 epoch 51 - iter 801/891 - loss 0.07469760\n",
            "2019-06-08 14:18:37,486 epoch 51 - iter 890/891 - loss 0.07593131\n",
            "2019-06-08 14:18:38,442 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:18:38,444 EPOCH 51 done: loss 0.0759 - lr 0.0031 - bad epochs 0\n",
            "2019-06-08 14:18:59,889 DEV : loss 0.11004388332366943 - score 0.9283\n",
            "2019-06-08 14:20:30,714 TEST : loss 0.10536840558052063 - score 0.9192\n",
            "2019-06-08 14:20:36,001 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:20:37,123 epoch 52 - iter 0/891 - loss 0.01313710\n",
            "2019-06-08 14:20:56,926 epoch 52 - iter 89/891 - loss 0.08192689\n",
            "2019-06-08 14:21:16,324 epoch 52 - iter 178/891 - loss 0.07786827\n",
            "2019-06-08 14:21:37,929 epoch 52 - iter 267/891 - loss 0.07854950\n",
            "2019-06-08 14:21:56,759 epoch 52 - iter 356/891 - loss 0.07453411\n",
            "2019-06-08 14:22:15,194 epoch 52 - iter 445/891 - loss 0.07241914\n",
            "2019-06-08 14:22:33,652 epoch 52 - iter 534/891 - loss 0.07266470\n",
            "2019-06-08 14:22:54,084 epoch 52 - iter 623/891 - loss 0.07366565\n",
            "2019-06-08 14:23:12,762 epoch 52 - iter 712/891 - loss 0.07541729\n",
            "2019-06-08 14:23:31,275 epoch 52 - iter 801/891 - loss 0.07561834\n",
            "2019-06-08 14:23:51,765 epoch 52 - iter 890/891 - loss 0.07527270\n",
            "2019-06-08 14:23:52,672 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:23:52,676 EPOCH 52 done: loss 0.0753 - lr 0.0031 - bad epochs 0\n",
            "2019-06-08 14:24:14,137 DEV : loss 0.11007214337587357 - score 0.929\n",
            "2019-06-08 14:25:42,964 TEST : loss 0.10578523576259613 - score 0.9192\n",
            "2019-06-08 14:25:48,272 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:25:49,408 epoch 53 - iter 0/891 - loss 0.12372398\n",
            "2019-06-08 14:26:08,809 epoch 53 - iter 89/891 - loss 0.08266584\n",
            "2019-06-08 14:26:30,605 epoch 53 - iter 178/891 - loss 0.08396290\n",
            "2019-06-08 14:26:49,583 epoch 53 - iter 267/891 - loss 0.07700352\n",
            "2019-06-08 14:27:08,112 epoch 53 - iter 356/891 - loss 0.07732541\n",
            "2019-06-08 14:27:28,933 epoch 53 - iter 445/891 - loss 0.07687220\n",
            "2019-06-08 14:27:47,595 epoch 53 - iter 534/891 - loss 0.07650190\n",
            "2019-06-08 14:28:06,021 epoch 53 - iter 623/891 - loss 0.07464455\n",
            "2019-06-08 14:28:26,371 epoch 53 - iter 712/891 - loss 0.07546348\n",
            "2019-06-08 14:28:45,006 epoch 53 - iter 801/891 - loss 0.07517652\n",
            "2019-06-08 14:29:03,426 epoch 53 - iter 890/891 - loss 0.07458846\n",
            "2019-06-08 14:29:04,374 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:29:04,380 EPOCH 53 done: loss 0.0746 - lr 0.0031 - bad epochs 0\n",
            "2019-06-08 14:29:25,839 DEV : loss 0.11043901741504669 - score 0.9288\n",
            "2019-06-08 14:30:56,567 TEST : loss 0.10620163381099701 - score 0.9198\n",
            "2019-06-08 14:30:56,576 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:30:57,588 epoch 54 - iter 0/891 - loss 0.04251188\n",
            "2019-06-08 14:31:16,875 epoch 54 - iter 89/891 - loss 0.07450706\n",
            "2019-06-08 14:31:36,187 epoch 54 - iter 178/891 - loss 0.07198479\n",
            "2019-06-08 14:31:57,751 epoch 54 - iter 267/891 - loss 0.07297498\n",
            "2019-06-08 14:32:16,436 epoch 54 - iter 356/891 - loss 0.07210463\n",
            "2019-06-08 14:32:34,967 epoch 54 - iter 445/891 - loss 0.07249528\n",
            "2019-06-08 14:32:53,709 epoch 54 - iter 534/891 - loss 0.07409063\n",
            "2019-06-08 14:33:14,302 epoch 54 - iter 623/891 - loss 0.07458786\n",
            "2019-06-08 14:33:32,842 epoch 54 - iter 712/891 - loss 0.07293791\n",
            "2019-06-08 14:33:51,548 epoch 54 - iter 801/891 - loss 0.07342073\n",
            "2019-06-08 14:34:11,827 epoch 54 - iter 890/891 - loss 0.07373034\n",
            "2019-06-08 14:34:12,779 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:34:12,786 EPOCH 54 done: loss 0.0737 - lr 0.0031 - bad epochs 1\n",
            "2019-06-08 14:34:34,337 DEV : loss 0.1109413355588913 - score 0.9273\n",
            "2019-06-08 14:36:03,164 TEST : loss 0.10587427765130997 - score 0.9212\n",
            "2019-06-08 14:36:03,172 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:36:04,140 epoch 55 - iter 0/891 - loss 0.00301835\n",
            "2019-06-08 14:36:26,113 epoch 55 - iter 89/891 - loss 0.07434933\n",
            "2019-06-08 14:36:45,256 epoch 55 - iter 178/891 - loss 0.06702018\n",
            "2019-06-08 14:37:04,312 epoch 55 - iter 267/891 - loss 0.06817020\n",
            "2019-06-08 14:37:23,196 epoch 55 - iter 356/891 - loss 0.06882475\n",
            "2019-06-08 14:37:43,812 epoch 55 - iter 445/891 - loss 0.06977173\n",
            "2019-06-08 14:38:02,485 epoch 55 - iter 534/891 - loss 0.07029784\n",
            "2019-06-08 14:38:20,828 epoch 55 - iter 623/891 - loss 0.07156464\n",
            "2019-06-08 14:38:41,130 epoch 55 - iter 712/891 - loss 0.07155432\n",
            "2019-06-08 14:38:59,722 epoch 55 - iter 801/891 - loss 0.07120025\n",
            "2019-06-08 14:39:18,270 epoch 55 - iter 890/891 - loss 0.07140250\n",
            "2019-06-08 14:39:19,200 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:39:19,207 EPOCH 55 done: loss 0.0714 - lr 0.0031 - bad epochs 2\n",
            "2019-06-08 14:39:43,309 DEV : loss 0.11086754500865936 - score 0.9257\n",
            "2019-06-08 14:41:11,929 TEST : loss 0.10609705746173859 - score 0.92\n",
            "2019-06-08 14:41:11,937 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:41:12,935 epoch 56 - iter 0/891 - loss 0.17947224\n",
            "2019-06-08 14:41:32,210 epoch 56 - iter 89/891 - loss 0.08043934\n",
            "2019-06-08 14:41:51,522 epoch 56 - iter 178/891 - loss 0.07883614\n",
            "2019-06-08 14:42:12,793 epoch 56 - iter 267/891 - loss 0.07848505\n",
            "2019-06-08 14:42:31,529 epoch 56 - iter 356/891 - loss 0.07900419\n",
            "2019-06-08 14:42:50,249 epoch 56 - iter 445/891 - loss 0.07669677\n",
            "2019-06-08 14:43:10,832 epoch 56 - iter 534/891 - loss 0.07601271\n",
            "2019-06-08 14:43:29,351 epoch 56 - iter 623/891 - loss 0.07658811\n",
            "2019-06-08 14:43:47,940 epoch 56 - iter 712/891 - loss 0.07505402\n",
            "2019-06-08 14:44:06,490 epoch 56 - iter 801/891 - loss 0.07480433\n",
            "2019-06-08 14:44:26,763 epoch 56 - iter 890/891 - loss 0.07603662\n",
            "2019-06-08 14:44:27,713 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:44:27,720 EPOCH 56 done: loss 0.0760 - lr 0.0031 - bad epochs 3\n",
            "2019-06-08 14:44:49,335 DEV : loss 0.11074855178594589 - score 0.9271\n",
            "2019-06-08 14:46:18,163 TEST : loss 0.10634735971689224 - score 0.9198\n",
            "Epoch    55: reducing learning rate of group 0 to 1.5625e-03.\n",
            "2019-06-08 14:46:18,171 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:46:19,136 epoch 57 - iter 0/891 - loss 0.06109911\n",
            "2019-06-08 14:46:40,848 epoch 57 - iter 89/891 - loss 0.07227467\n",
            "2019-06-08 14:46:59,783 epoch 57 - iter 178/891 - loss 0.06890930\n",
            "2019-06-08 14:47:18,802 epoch 57 - iter 267/891 - loss 0.07047364\n",
            "2019-06-08 14:47:39,569 epoch 57 - iter 356/891 - loss 0.07287041\n",
            "2019-06-08 14:47:58,168 epoch 57 - iter 445/891 - loss 0.07430483\n",
            "2019-06-08 14:48:16,797 epoch 57 - iter 534/891 - loss 0.07516560\n",
            "2019-06-08 14:48:35,468 epoch 57 - iter 623/891 - loss 0.07528157\n",
            "2019-06-08 14:48:55,882 epoch 57 - iter 712/891 - loss 0.07458460\n",
            "2019-06-08 14:49:14,471 epoch 57 - iter 801/891 - loss 0.07504458\n",
            "2019-06-08 14:49:33,117 epoch 57 - iter 890/891 - loss 0.07419097\n",
            "2019-06-08 14:49:34,037 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:49:34,042 EPOCH 57 done: loss 0.0742 - lr 0.0016 - bad epochs 0\n",
            "2019-06-08 14:49:58,322 DEV : loss 0.11097320169210434 - score 0.9282\n",
            "2019-06-08 14:51:26,918 TEST : loss 0.10614292323589325 - score 0.9203\n",
            "2019-06-08 14:51:26,926 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:51:27,900 epoch 58 - iter 0/891 - loss 0.02169278\n",
            "2019-06-08 14:51:47,242 epoch 58 - iter 89/891 - loss 0.07788743\n",
            "2019-06-08 14:52:08,711 epoch 58 - iter 178/891 - loss 0.07196649\n",
            "2019-06-08 14:52:27,439 epoch 58 - iter 267/891 - loss 0.07471499\n",
            "2019-06-08 14:52:46,420 epoch 58 - iter 356/891 - loss 0.07472851\n",
            "2019-06-08 14:53:04,978 epoch 58 - iter 445/891 - loss 0.07506133\n",
            "2019-06-08 14:53:25,738 epoch 58 - iter 534/891 - loss 0.07441030\n",
            "2019-06-08 14:53:44,329 epoch 58 - iter 623/891 - loss 0.07443636\n",
            "2019-06-08 14:54:02,883 epoch 58 - iter 712/891 - loss 0.07400845\n",
            "2019-06-08 14:54:23,241 epoch 58 - iter 801/891 - loss 0.07540436\n",
            "2019-06-08 14:54:41,771 epoch 58 - iter 890/891 - loss 0.07564177\n",
            "2019-06-08 14:54:42,729 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:54:42,731 EPOCH 58 done: loss 0.0756 - lr 0.0016 - bad epochs 1\n",
            "2019-06-08 14:55:04,229 DEV : loss 0.11142615228891373 - score 0.9272\n",
            "2019-06-08 14:56:33,114 TEST : loss 0.10624027997255325 - score 0.9207\n",
            "2019-06-08 14:56:33,123 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:56:34,105 epoch 59 - iter 0/891 - loss 0.18022147\n",
            "2019-06-08 14:56:56,083 epoch 59 - iter 89/891 - loss 0.07777373\n",
            "2019-06-08 14:57:14,929 epoch 59 - iter 178/891 - loss 0.07948255\n",
            "2019-06-08 14:57:33,874 epoch 59 - iter 267/891 - loss 0.07714663\n",
            "2019-06-08 14:57:54,867 epoch 59 - iter 356/891 - loss 0.07559578\n",
            "2019-06-08 14:58:13,572 epoch 59 - iter 445/891 - loss 0.07616570\n",
            "2019-06-08 14:58:32,082 epoch 59 - iter 534/891 - loss 0.07704119\n",
            "2019-06-08 14:58:50,594 epoch 59 - iter 623/891 - loss 0.07508014\n",
            "2019-06-08 14:59:11,152 epoch 59 - iter 712/891 - loss 0.07350544\n",
            "2019-06-08 14:59:29,835 epoch 59 - iter 801/891 - loss 0.07274916\n",
            "2019-06-08 14:59:48,310 epoch 59 - iter 890/891 - loss 0.07336548\n",
            "2019-06-08 14:59:49,237 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 14:59:49,242 EPOCH 59 done: loss 0.0734 - lr 0.0016 - bad epochs 2\n",
            "2019-06-08 15:00:13,536 DEV : loss 0.11117296665906906 - score 0.9264\n",
            "2019-06-08 15:01:42,388 TEST : loss 0.10648004710674286 - score 0.9189\n",
            "2019-06-08 15:01:42,397 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:01:43,342 epoch 60 - iter 0/891 - loss 0.18983665\n",
            "2019-06-08 15:02:02,723 epoch 60 - iter 89/891 - loss 0.07992703\n",
            "2019-06-08 15:02:24,413 epoch 60 - iter 178/891 - loss 0.07059598\n",
            "2019-06-08 15:02:43,291 epoch 60 - iter 267/891 - loss 0.07812536\n",
            "2019-06-08 15:03:02,097 epoch 60 - iter 356/891 - loss 0.07701109\n",
            "2019-06-08 15:03:20,602 epoch 60 - iter 445/891 - loss 0.07537475\n",
            "2019-06-08 15:03:41,144 epoch 60 - iter 534/891 - loss 0.07439961\n",
            "2019-06-08 15:03:59,684 epoch 60 - iter 623/891 - loss 0.07558690\n",
            "2019-06-08 15:04:18,272 epoch 60 - iter 712/891 - loss 0.07359932\n",
            "2019-06-08 15:04:38,796 epoch 60 - iter 801/891 - loss 0.07366980\n",
            "2019-06-08 15:04:57,226 epoch 60 - iter 890/891 - loss 0.07467081\n",
            "2019-06-08 15:04:58,196 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:04:58,203 EPOCH 60 done: loss 0.0747 - lr 0.0016 - bad epochs 3\n",
            "2019-06-08 15:05:19,672 DEV : loss 0.1115739643573761 - score 0.9272\n",
            "2019-06-08 15:06:50,631 TEST : loss 0.10646466165781021 - score 0.9203\n",
            "Epoch    59: reducing learning rate of group 0 to 7.8125e-04.\n",
            "2019-06-08 15:06:50,640 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:06:51,643 epoch 61 - iter 0/891 - loss 0.01937614\n",
            "2019-06-08 15:07:11,180 epoch 61 - iter 89/891 - loss 0.05904774\n",
            "2019-06-08 15:07:30,371 epoch 61 - iter 178/891 - loss 0.06630653\n",
            "2019-06-08 15:07:49,545 epoch 61 - iter 267/891 - loss 0.06897472\n",
            "2019-06-08 15:08:10,598 epoch 61 - iter 356/891 - loss 0.06989539\n",
            "2019-06-08 15:08:29,265 epoch 61 - iter 445/891 - loss 0.07263912\n",
            "2019-06-08 15:08:47,904 epoch 61 - iter 534/891 - loss 0.07140353\n",
            "2019-06-08 15:09:08,510 epoch 61 - iter 623/891 - loss 0.07201492\n",
            "2019-06-08 15:09:27,031 epoch 61 - iter 712/891 - loss 0.07162856\n",
            "2019-06-08 15:09:45,537 epoch 61 - iter 801/891 - loss 0.07344770\n",
            "2019-06-08 15:10:03,846 epoch 61 - iter 890/891 - loss 0.07348975\n",
            "2019-06-08 15:10:04,779 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:10:04,781 EPOCH 61 done: loss 0.0735 - lr 0.0008 - bad epochs 0\n",
            "2019-06-08 15:10:29,184 DEV : loss 0.11157689988613129 - score 0.9265\n",
            "2019-06-08 15:11:58,133 TEST : loss 0.10638003796339035 - score 0.9201\n",
            "2019-06-08 15:11:58,142 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:11:59,122 epoch 62 - iter 0/891 - loss 0.26782557\n",
            "2019-06-08 15:12:18,353 epoch 62 - iter 89/891 - loss 0.08703510\n",
            "2019-06-08 15:12:39,988 epoch 62 - iter 178/891 - loss 0.08704301\n",
            "2019-06-08 15:12:58,827 epoch 62 - iter 267/891 - loss 0.08535454\n",
            "2019-06-08 15:13:17,404 epoch 62 - iter 356/891 - loss 0.07951530\n",
            "2019-06-08 15:13:38,273 epoch 62 - iter 445/891 - loss 0.08103696\n",
            "2019-06-08 15:13:56,790 epoch 62 - iter 534/891 - loss 0.07920017\n",
            "2019-06-08 15:14:15,502 epoch 62 - iter 623/891 - loss 0.07911068\n",
            "2019-06-08 15:14:33,904 epoch 62 - iter 712/891 - loss 0.07744007\n",
            "2019-06-08 15:14:54,649 epoch 62 - iter 801/891 - loss 0.07688775\n",
            "2019-06-08 15:15:13,177 epoch 62 - iter 890/891 - loss 0.07688840\n",
            "2019-06-08 15:15:14,125 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:15:14,133 EPOCH 62 done: loss 0.0769 - lr 0.0008 - bad epochs 1\n",
            "2019-06-08 15:15:35,628 DEV : loss 0.11162739247083664 - score 0.9257\n",
            "2019-06-08 15:17:06,402 TEST : loss 0.1065768450498581 - score 0.9184\n",
            "2019-06-08 15:17:06,410 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:17:07,398 epoch 63 - iter 0/891 - loss 0.09992400\n",
            "2019-06-08 15:17:26,577 epoch 63 - iter 89/891 - loss 0.08769393\n",
            "2019-06-08 15:17:45,563 epoch 63 - iter 178/891 - loss 0.07973179\n",
            "2019-06-08 15:18:04,878 epoch 63 - iter 267/891 - loss 0.07597587\n",
            "2019-06-08 15:18:26,169 epoch 63 - iter 356/891 - loss 0.07485504\n",
            "2019-06-08 15:18:44,925 epoch 63 - iter 445/891 - loss 0.07644749\n",
            "2019-06-08 15:19:03,589 epoch 63 - iter 534/891 - loss 0.07929382\n",
            "2019-06-08 15:19:24,184 epoch 63 - iter 623/891 - loss 0.07849113\n",
            "2019-06-08 15:19:42,703 epoch 63 - iter 712/891 - loss 0.07661824\n",
            "2019-06-08 15:20:01,141 epoch 63 - iter 801/891 - loss 0.07821771\n",
            "2019-06-08 15:20:19,445 epoch 63 - iter 890/891 - loss 0.07745318\n",
            "2019-06-08 15:20:20,381 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:20:20,387 EPOCH 63 done: loss 0.0775 - lr 0.0008 - bad epochs 2\n",
            "2019-06-08 15:20:44,726 DEV : loss 0.11168789118528366 - score 0.9264\n",
            "2019-06-08 15:22:13,661 TEST : loss 0.10672646015882492 - score 0.9195\n",
            "2019-06-08 15:22:13,669 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:22:14,642 epoch 64 - iter 0/891 - loss 0.02606218\n",
            "2019-06-08 15:22:33,841 epoch 64 - iter 89/891 - loss 0.07237232\n",
            "2019-06-08 15:22:55,398 epoch 64 - iter 178/891 - loss 0.07579666\n",
            "2019-06-08 15:23:14,375 epoch 64 - iter 267/891 - loss 0.07847623\n",
            "2019-06-08 15:23:33,140 epoch 64 - iter 356/891 - loss 0.07765761\n",
            "2019-06-08 15:23:53,802 epoch 64 - iter 445/891 - loss 0.07774852\n",
            "2019-06-08 15:24:12,358 epoch 64 - iter 534/891 - loss 0.07525465\n",
            "2019-06-08 15:24:30,881 epoch 64 - iter 623/891 - loss 0.07376759\n",
            "2019-06-08 15:24:49,557 epoch 64 - iter 712/891 - loss 0.07425142\n",
            "2019-06-08 15:25:09,941 epoch 64 - iter 801/891 - loss 0.07570169\n",
            "2019-06-08 15:25:28,241 epoch 64 - iter 890/891 - loss 0.07589562\n",
            "2019-06-08 15:25:29,172 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:25:29,173 EPOCH 64 done: loss 0.0759 - lr 0.0008 - bad epochs 3\n",
            "2019-06-08 15:25:50,661 DEV : loss 0.11154686659574509 - score 0.9257\n",
            "2019-06-08 15:27:21,329 TEST : loss 0.1066758930683136 - score 0.9184\n",
            "Epoch    63: reducing learning rate of group 0 to 3.9063e-04.\n",
            "2019-06-08 15:27:21,337 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:27:22,304 epoch 65 - iter 0/891 - loss 0.16885245\n",
            "2019-06-08 15:27:41,628 epoch 65 - iter 89/891 - loss 0.07758009\n",
            "2019-06-08 15:28:00,874 epoch 65 - iter 178/891 - loss 0.08069240\n",
            "2019-06-08 15:28:22,121 epoch 65 - iter 267/891 - loss 0.07650997\n",
            "2019-06-08 15:28:40,852 epoch 65 - iter 356/891 - loss 0.07664581\n",
            "2019-06-08 15:28:59,655 epoch 65 - iter 445/891 - loss 0.07404242\n",
            "2019-06-08 15:29:18,309 epoch 65 - iter 534/891 - loss 0.07392584\n",
            "2019-06-08 15:29:38,815 epoch 65 - iter 623/891 - loss 0.07369214\n",
            "2019-06-08 15:29:57,348 epoch 65 - iter 712/891 - loss 0.07280540\n",
            "2019-06-08 15:30:16,005 epoch 65 - iter 801/891 - loss 0.07366719\n",
            "2019-06-08 15:30:36,304 epoch 65 - iter 890/891 - loss 0.07379421\n",
            "2019-06-08 15:30:37,243 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:30:37,244 EPOCH 65 done: loss 0.0738 - lr 0.0004 - bad epochs 0\n",
            "2019-06-08 15:30:58,747 DEV : loss 0.11143854260444641 - score 0.9257\n",
            "2019-06-08 15:32:27,423 TEST : loss 0.10665855556726456 - score 0.9184\n",
            "2019-06-08 15:32:27,431 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:32:28,436 epoch 66 - iter 0/891 - loss 0.04962325\n",
            "2019-06-08 15:32:47,486 epoch 66 - iter 89/891 - loss 0.09565410\n",
            "2019-06-08 15:33:09,146 epoch 66 - iter 178/891 - loss 0.08935987\n",
            "2019-06-08 15:33:28,248 epoch 66 - iter 267/891 - loss 0.08341346\n",
            "2019-06-08 15:33:46,855 epoch 66 - iter 356/891 - loss 0.08258911\n",
            "2019-06-08 15:34:07,947 epoch 66 - iter 445/891 - loss 0.07586212\n",
            "2019-06-08 15:34:26,605 epoch 66 - iter 534/891 - loss 0.07736062\n",
            "2019-06-08 15:34:45,415 epoch 66 - iter 623/891 - loss 0.07633836\n",
            "2019-06-08 15:35:05,979 epoch 66 - iter 712/891 - loss 0.07629433\n",
            "2019-06-08 15:35:24,764 epoch 66 - iter 801/891 - loss 0.07492254\n",
            "2019-06-08 15:35:43,405 epoch 66 - iter 890/891 - loss 0.07519937\n",
            "2019-06-08 15:35:44,328 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:35:44,335 EPOCH 66 done: loss 0.0752 - lr 0.0004 - bad epochs 1\n",
            "2019-06-08 15:36:05,860 DEV : loss 0.11139768362045288 - score 0.9257\n",
            "2019-06-08 15:37:37,073 TEST : loss 0.10664401948451996 - score 0.9193\n",
            "2019-06-08 15:37:37,083 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:37:38,075 epoch 67 - iter 0/891 - loss 0.19180202\n",
            "2019-06-08 15:37:57,606 epoch 67 - iter 89/891 - loss 0.07075402\n",
            "2019-06-08 15:38:17,002 epoch 67 - iter 178/891 - loss 0.07179103\n",
            "2019-06-08 15:38:38,552 epoch 67 - iter 267/891 - loss 0.07527748\n",
            "2019-06-08 15:38:57,439 epoch 67 - iter 356/891 - loss 0.07265752\n",
            "2019-06-08 15:39:16,195 epoch 67 - iter 445/891 - loss 0.07372257\n",
            "2019-06-08 15:39:34,973 epoch 67 - iter 534/891 - loss 0.07320523\n",
            "2019-06-08 15:39:56,044 epoch 67 - iter 623/891 - loss 0.07317671\n",
            "2019-06-08 15:40:14,811 epoch 67 - iter 712/891 - loss 0.07388165\n",
            "2019-06-08 15:40:33,347 epoch 67 - iter 801/891 - loss 0.07475813\n",
            "2019-06-08 15:40:53,783 epoch 67 - iter 890/891 - loss 0.07524034\n",
            "2019-06-08 15:40:54,711 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:40:54,713 EPOCH 67 done: loss 0.0752 - lr 0.0004 - bad epochs 2\n",
            "2019-06-08 15:41:16,412 DEV : loss 0.11129419505596161 - score 0.9264\n",
            "2019-06-08 15:42:45,741 TEST : loss 0.10669251531362534 - score 0.919\n",
            "2019-06-08 15:42:45,749 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:42:46,757 epoch 68 - iter 0/891 - loss 0.08545715\n",
            "2019-06-08 15:43:08,735 epoch 68 - iter 89/891 - loss 0.08333328\n",
            "2019-06-08 15:43:27,686 epoch 68 - iter 178/891 - loss 0.07759716\n",
            "2019-06-08 15:43:46,714 epoch 68 - iter 267/891 - loss 0.07601601\n",
            "2019-06-08 15:44:05,741 epoch 68 - iter 356/891 - loss 0.07956322\n",
            "2019-06-08 15:44:26,854 epoch 68 - iter 445/891 - loss 0.07827222\n",
            "2019-06-08 15:44:45,493 epoch 68 - iter 534/891 - loss 0.07770412\n",
            "2019-06-08 15:45:04,341 epoch 68 - iter 623/891 - loss 0.07655193\n",
            "2019-06-08 15:45:24,992 epoch 68 - iter 712/891 - loss 0.07707571\n",
            "2019-06-08 15:45:43,617 epoch 68 - iter 801/891 - loss 0.07635868\n",
            "2019-06-08 15:46:02,326 epoch 68 - iter 890/891 - loss 0.07520817\n",
            "2019-06-08 15:46:03,289 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:46:03,291 EPOCH 68 done: loss 0.0752 - lr 0.0004 - bad epochs 3\n",
            "2019-06-08 15:46:25,001 DEV : loss 0.11139112710952759 - score 0.9257\n",
            "2019-06-08 15:47:56,312 TEST : loss 0.1066950112581253 - score 0.9194\n",
            "Epoch    67: reducing learning rate of group 0 to 1.9531e-04.\n",
            "2019-06-08 15:47:56,320 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:47:57,301 epoch 69 - iter 0/891 - loss 0.15605801\n",
            "2019-06-08 15:48:16,591 epoch 69 - iter 89/891 - loss 0.06036316\n",
            "2019-06-08 15:48:35,909 epoch 69 - iter 178/891 - loss 0.07802312\n",
            "2019-06-08 15:48:57,606 epoch 69 - iter 267/891 - loss 0.07704093\n",
            "2019-06-08 15:49:16,389 epoch 69 - iter 356/891 - loss 0.07315759\n",
            "2019-06-08 15:49:34,962 epoch 69 - iter 445/891 - loss 0.07102413\n",
            "2019-06-08 15:49:53,849 epoch 69 - iter 534/891 - loss 0.07047178\n",
            "2019-06-08 15:50:14,531 epoch 69 - iter 623/891 - loss 0.07066120\n",
            "2019-06-08 15:50:33,265 epoch 69 - iter 712/891 - loss 0.07259739\n",
            "2019-06-08 15:50:52,041 epoch 69 - iter 801/891 - loss 0.07216128\n",
            "2019-06-08 15:51:12,729 epoch 69 - iter 890/891 - loss 0.07188533\n",
            "2019-06-08 15:51:13,701 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:51:13,709 EPOCH 69 done: loss 0.0719 - lr 0.0002 - bad epochs 0\n",
            "2019-06-08 15:51:35,367 DEV : loss 0.11142843216657639 - score 0.9249\n",
            "2019-06-08 15:53:04,894 TEST : loss 0.10664807260036469 - score 0.9191\n",
            "2019-06-08 15:53:04,906 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:53:05,897 epoch 70 - iter 0/891 - loss 0.06785764\n",
            "2019-06-08 15:53:27,923 epoch 70 - iter 89/891 - loss 0.06965720\n",
            "2019-06-08 15:53:46,990 epoch 70 - iter 178/891 - loss 0.06636315\n",
            "2019-06-08 15:54:06,220 epoch 70 - iter 267/891 - loss 0.06976268\n",
            "2019-06-08 15:54:27,347 epoch 70 - iter 356/891 - loss 0.07256955\n",
            "2019-06-08 15:54:46,071 epoch 70 - iter 445/891 - loss 0.07260489\n",
            "2019-06-08 15:55:04,882 epoch 70 - iter 534/891 - loss 0.07469300\n",
            "2019-06-08 15:55:23,611 epoch 70 - iter 623/891 - loss 0.07207128\n",
            "2019-06-08 15:55:44,242 epoch 70 - iter 712/891 - loss 0.07223390\n",
            "2019-06-08 15:56:02,975 epoch 70 - iter 801/891 - loss 0.07253195\n",
            "2019-06-08 15:56:21,386 epoch 70 - iter 890/891 - loss 0.07497686\n",
            "2019-06-08 15:56:22,341 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:56:22,347 EPOCH 70 done: loss 0.0750 - lr 0.0002 - bad epochs 1\n",
            "2019-06-08 15:56:46,663 DEV : loss 0.11140993237495422 - score 0.9257\n",
            "2019-06-08 15:58:16,111 TEST : loss 0.10665819048881531 - score 0.9193\n",
            "2019-06-08 15:58:16,119 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 15:58:17,115 epoch 71 - iter 0/891 - loss 0.05288438\n",
            "2019-06-08 15:58:36,595 epoch 71 - iter 89/891 - loss 0.07855693\n",
            "2019-06-08 15:58:56,062 epoch 71 - iter 178/891 - loss 0.07752134\n",
            "2019-06-08 15:59:17,582 epoch 71 - iter 267/891 - loss 0.07550282\n",
            "2019-06-08 15:59:36,463 epoch 71 - iter 356/891 - loss 0.07410057\n",
            "2019-06-08 15:59:55,209 epoch 71 - iter 445/891 - loss 0.07629504\n",
            "2019-06-08 16:00:15,876 epoch 71 - iter 534/891 - loss 0.07840376\n",
            "2019-06-08 16:00:34,540 epoch 71 - iter 623/891 - loss 0.07854036\n",
            "2019-06-08 16:00:53,290 epoch 71 - iter 712/891 - loss 0.07813640\n",
            "2019-06-08 16:01:12,156 epoch 71 - iter 801/891 - loss 0.07769761\n",
            "2019-06-08 16:01:32,722 epoch 71 - iter 890/891 - loss 0.07789162\n",
            "2019-06-08 16:01:33,684 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 16:01:33,686 EPOCH 71 done: loss 0.0779 - lr 0.0002 - bad epochs 2\n",
            "2019-06-08 16:01:55,385 DEV : loss 0.11139777302742004 - score 0.9257\n",
            "2019-06-08 16:03:24,844 TEST : loss 0.10664910078048706 - score 0.9193\n",
            "2019-06-08 16:03:24,853 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 16:03:25,844 epoch 72 - iter 0/891 - loss 0.01062390\n",
            "2019-06-08 16:03:48,142 epoch 72 - iter 89/891 - loss 0.07024441\n",
            "2019-06-08 16:04:07,207 epoch 72 - iter 178/891 - loss 0.06963755\n",
            "2019-06-08 16:04:26,178 epoch 72 - iter 267/891 - loss 0.06867667\n",
            "2019-06-08 16:04:47,124 epoch 72 - iter 356/891 - loss 0.07327423\n",
            "2019-06-08 16:05:05,924 epoch 72 - iter 445/891 - loss 0.07270675\n",
            "2019-06-08 16:05:24,711 epoch 72 - iter 534/891 - loss 0.07347069\n",
            "2019-06-08 16:05:43,561 epoch 72 - iter 623/891 - loss 0.07263960\n",
            "2019-06-08 16:06:04,402 epoch 72 - iter 712/891 - loss 0.07226448\n",
            "2019-06-08 16:06:22,940 epoch 72 - iter 801/891 - loss 0.07242441\n",
            "2019-06-08 16:06:41,764 epoch 72 - iter 890/891 - loss 0.07331155\n",
            "2019-06-08 16:06:42,717 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 16:06:42,724 EPOCH 72 done: loss 0.0733 - lr 0.0002 - bad epochs 3\n",
            "2019-06-08 16:07:07,013 DEV : loss 0.11134213954210281 - score 0.9257\n",
            "2019-06-08 16:08:36,511 TEST : loss 0.10663426667451859 - score 0.9193\n",
            "Epoch    71: reducing learning rate of group 0 to 9.7656e-05.\n",
            "2019-06-08 16:08:36,520 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 16:08:36,522 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 16:08:36,528 learning rate too small - quitting training!\n",
            "2019-06-08 16:08:36,532 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 16:08:41,843 ----------------------------------------------------------------------------------------------------\n",
            "2019-06-08 16:08:41,851 Testing using best model ...\n",
            "2019-06-08 16:08:41,859 loading file resources/taggers/resume-ner/best-model.pt\n",
            "2019-06-08 16:10:15,533 0.9274\t0.9111\t0.9192\n",
            "2019-06-08 16:10:15,535 \n",
            "MICRO_AVG: acc 0.8504 - f1-score 0.9192\n",
            "MACRO_AVG: acc 0.7907 - f1-score 0.86755\n",
            "\"B-Companies tp: 464 - fp: 0 - fn: 0 - tn: 464 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
            "\"I-Companies tp: 1282 - fp: 0 - fn: 0 - tn: 1282 - precision: 1.0000 - recall: 1.0000 - accuracy: 1.0000 - f1-score: 1.0000\n",
            "Degree     tp: 154 - fp: 44 - fn: 59 - tn: 154 - precision: 0.7778 - recall: 0.7230 - accuracy: 0.5992 - f1-score: 0.7494\n",
            "Designation tp: 386 - fp: 135 - fn: 164 - tn: 386 - precision: 0.7409 - recall: 0.7018 - accuracy: 0.5635 - f1-score: 0.7208\n",
            "2019-06-08 16:10:15,544 ----------------------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dev_loss_history': [tensor(0.2777, device='cuda:0'),\n",
              "  tensor(0.2024, device='cuda:0'),\n",
              "  tensor(0.1776, device='cuda:0'),\n",
              "  tensor(0.1826, device='cuda:0'),\n",
              "  tensor(0.1698, device='cuda:0'),\n",
              "  tensor(0.1593, device='cuda:0'),\n",
              "  tensor(0.1529, device='cuda:0'),\n",
              "  tensor(0.1406, device='cuda:0'),\n",
              "  tensor(0.1393, device='cuda:0'),\n",
              "  tensor(0.1382, device='cuda:0'),\n",
              "  tensor(0.1335, device='cuda:0'),\n",
              "  tensor(0.1275, device='cuda:0'),\n",
              "  tensor(0.1269, device='cuda:0'),\n",
              "  tensor(0.1349, device='cuda:0'),\n",
              "  tensor(0.1234, device='cuda:0'),\n",
              "  tensor(0.1219, device='cuda:0'),\n",
              "  tensor(0.1312, device='cuda:0'),\n",
              "  tensor(0.1273, device='cuda:0'),\n",
              "  tensor(0.1178, device='cuda:0'),\n",
              "  tensor(0.1329, device='cuda:0'),\n",
              "  tensor(0.1176, device='cuda:0'),\n",
              "  tensor(0.1318, device='cuda:0'),\n",
              "  tensor(0.1184, device='cuda:0'),\n",
              "  tensor(0.1214, device='cuda:0'),\n",
              "  tensor(0.1175, device='cuda:0'),\n",
              "  tensor(0.1264, device='cuda:0'),\n",
              "  tensor(0.1174, device='cuda:0'),\n",
              "  tensor(0.1246, device='cuda:0'),\n",
              "  tensor(0.1173, device='cuda:0'),\n",
              "  tensor(0.1181, device='cuda:0'),\n",
              "  tensor(0.1148, device='cuda:0'),\n",
              "  tensor(0.1143, device='cuda:0'),\n",
              "  tensor(0.1124, device='cuda:0'),\n",
              "  tensor(0.1130, device='cuda:0'),\n",
              "  tensor(0.1130, device='cuda:0'),\n",
              "  tensor(0.1137, device='cuda:0'),\n",
              "  tensor(0.1107, device='cuda:0'),\n",
              "  tensor(0.1153, device='cuda:0'),\n",
              "  tensor(0.1101, device='cuda:0'),\n",
              "  tensor(0.1110, device='cuda:0'),\n",
              "  tensor(0.1098, device='cuda:0'),\n",
              "  tensor(0.1135, device='cuda:0'),\n",
              "  tensor(0.1124, device='cuda:0'),\n",
              "  tensor(0.1125, device='cuda:0'),\n",
              "  tensor(0.1110, device='cuda:0'),\n",
              "  tensor(0.1111, device='cuda:0'),\n",
              "  tensor(0.1110, device='cuda:0'),\n",
              "  tensor(0.1119, device='cuda:0'),\n",
              "  tensor(0.1118, device='cuda:0'),\n",
              "  tensor(0.1104, device='cuda:0'),\n",
              "  tensor(0.1100, device='cuda:0'),\n",
              "  tensor(0.1101, device='cuda:0'),\n",
              "  tensor(0.1104, device='cuda:0'),\n",
              "  tensor(0.1109, device='cuda:0'),\n",
              "  tensor(0.1109, device='cuda:0'),\n",
              "  tensor(0.1107, device='cuda:0'),\n",
              "  tensor(0.1110, device='cuda:0'),\n",
              "  tensor(0.1114, device='cuda:0'),\n",
              "  tensor(0.1112, device='cuda:0'),\n",
              "  tensor(0.1116, device='cuda:0'),\n",
              "  tensor(0.1116, device='cuda:0'),\n",
              "  tensor(0.1116, device='cuda:0'),\n",
              "  tensor(0.1117, device='cuda:0'),\n",
              "  tensor(0.1115, device='cuda:0'),\n",
              "  tensor(0.1114, device='cuda:0'),\n",
              "  tensor(0.1114, device='cuda:0'),\n",
              "  tensor(0.1113, device='cuda:0'),\n",
              "  tensor(0.1114, device='cuda:0'),\n",
              "  tensor(0.1114, device='cuda:0'),\n",
              "  tensor(0.1114, device='cuda:0'),\n",
              "  tensor(0.1114, device='cuda:0'),\n",
              "  tensor(0.1113, device='cuda:0')],\n",
              " 'dev_score_history': [0.8439,\n",
              "  0.8882,\n",
              "  0.8849,\n",
              "  0.8776,\n",
              "  0.8935,\n",
              "  0.8854,\n",
              "  0.8883,\n",
              "  0.8968,\n",
              "  0.9018,\n",
              "  0.9068,\n",
              "  0.9042,\n",
              "  0.9117,\n",
              "  0.9112,\n",
              "  0.9126,\n",
              "  0.9114,\n",
              "  0.9117,\n",
              "  0.9064,\n",
              "  0.9138,\n",
              "  0.9138,\n",
              "  0.9158,\n",
              "  0.9186,\n",
              "  0.9105,\n",
              "  0.9154,\n",
              "  0.9112,\n",
              "  0.9221,\n",
              "  0.9172,\n",
              "  0.9146,\n",
              "  0.9191,\n",
              "  0.9195,\n",
              "  0.9172,\n",
              "  0.9184,\n",
              "  0.9158,\n",
              "  0.9189,\n",
              "  0.9199,\n",
              "  0.9212,\n",
              "  0.9238,\n",
              "  0.9219,\n",
              "  0.9279,\n",
              "  0.9225,\n",
              "  0.9248,\n",
              "  0.927,\n",
              "  0.9252,\n",
              "  0.9276,\n",
              "  0.9247,\n",
              "  0.9249,\n",
              "  0.9268,\n",
              "  0.9262,\n",
              "  0.9275,\n",
              "  0.9271,\n",
              "  0.9275,\n",
              "  0.9283,\n",
              "  0.929,\n",
              "  0.9288,\n",
              "  0.9273,\n",
              "  0.9257,\n",
              "  0.9271,\n",
              "  0.9282,\n",
              "  0.9272,\n",
              "  0.9264,\n",
              "  0.9272,\n",
              "  0.9265,\n",
              "  0.9257,\n",
              "  0.9264,\n",
              "  0.9257,\n",
              "  0.9257,\n",
              "  0.9257,\n",
              "  0.9264,\n",
              "  0.9257,\n",
              "  0.9249,\n",
              "  0.9257,\n",
              "  0.9257,\n",
              "  0.9257],\n",
              " 'test_score': 0.9192,\n",
              " 'train_loss_history': [0.4931657323916887,\n",
              "  0.2469193363054232,\n",
              "  0.20867130088418154,\n",
              "  0.18775613921747766,\n",
              "  0.1746059202585959,\n",
              "  0.16244183471541346,\n",
              "  0.15782577466945072,\n",
              "  0.14931071495718828,\n",
              "  0.14289969960008106,\n",
              "  0.14325130521607252,\n",
              "  0.13793718243487113,\n",
              "  0.13201362800843988,\n",
              "  0.12678090106187898,\n",
              "  0.1257626585260987,\n",
              "  0.11936879346165995,\n",
              "  0.12243655495404127,\n",
              "  0.1234670044095428,\n",
              "  0.11677689360314614,\n",
              "  0.11317397189963145,\n",
              "  0.11273255488520653,\n",
              "  0.11148020055241188,\n",
              "  0.10832543826077183,\n",
              "  0.10377915591152027,\n",
              "  0.10630625056695532,\n",
              "  0.10499547899300368,\n",
              "  0.1052930130242808,\n",
              "  0.10200674462264904,\n",
              "  0.09776604188946897,\n",
              "  0.09701021643529001,\n",
              "  0.09182136593055708,\n",
              "  0.09126956009736659,\n",
              "  0.08783100417832918,\n",
              "  0.08947272334084767,\n",
              "  0.08470050338028556,\n",
              "  0.08268542484067773,\n",
              "  0.08082948518053583,\n",
              "  0.08334603633075183,\n",
              "  0.08040616361112841,\n",
              "  0.08152454241082767,\n",
              "  0.08363274354666657,\n",
              "  0.08096444693328154,\n",
              "  0.08011903404284323,\n",
              "  0.07879752560422519,\n",
              "  0.07807739022136671,\n",
              "  0.07846602505568402,\n",
              "  0.07865655697217393,\n",
              "  0.07991272574248823,\n",
              "  0.07717913212766818,\n",
              "  0.07661366422987814,\n",
              "  0.0760970026271378,\n",
              "  0.07593131226547037,\n",
              "  0.07527270048906907,\n",
              "  0.07458845842483336,\n",
              "  0.0737303397069475,\n",
              "  0.07140250313884916,\n",
              "  0.07603662361416498,\n",
              "  0.07419096673007816,\n",
              "  0.07564176692887116,\n",
              "  0.07336547637021736,\n",
              "  0.07467080511395244,\n",
              "  0.07348974629064371,\n",
              "  0.07688840494546559,\n",
              "  0.07745317509241076,\n",
              "  0.07589562312651772,\n",
              "  0.07379421362534215,\n",
              "  0.07519937355037858,\n",
              "  0.07524034273042422,\n",
              "  0.07520816703920183,\n",
              "  0.07188533467797753,\n",
              "  0.07497685694467071,\n",
              "  0.0778916167616886,\n",
              "  0.07331154781459558]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    }
  ]
}